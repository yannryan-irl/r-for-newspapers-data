[
["index.html", "A Short Guide to Historical Newspaper Data, Using R 1 Preface &amp; Acknowledgements", " A Short Guide to Historical Newspaper Data, Using R Yann Ryan 2020-09-24 1 Preface &amp; Acknowledgements I spent the last year or so working at the British Library as a Curator of Newspaper Data. It was an experimental position, conceived on the premise that the data generated by newspaper digitisation projects, in this case the Heritage Made Digital project, deserves its own curatorial role and set of practices. It was an fun time to work on such a project. While the large-scale analysis of newspaper data by historians is no longer in its infancy, it’s still probably in an adolescence, and it was exciting working within an evolving discipline, one which is developing its own set of practices, understanding where and how its bias works and the implications this might have, and generally moving from a sense of naive optimism to a practice which is more academically and mathematically rigorous and self-reflective. This book aims to give basic handbook for those who would like to dip their toes into big data analysis but don’t know where to begin. It takes advantage of the fact that for the first time, newspaper data from British Library is available to download, in bulk and for free. It uses some of that data, walking through the process from its download to running basic DH analyses: word frequencies, sentiment analysis, topic modelling and text reuse detection. It also introduces some mapping techniques, again using a dataset produced and made available by the British Library: a metadata list of the newspaper collection, which we hope will be of use to a wide variety of researchers. "],
["introduction.html", "2 Introduction 2.1 Unlocking the past with newspaper data 2.2 What can you do with newspaper data? 2.3 Goals 2.4 Why R? 2.5 Getting started 2.6 Who is the guide for? 2.7 Format of the book 2.8 Contribute", " 2 Introduction 2.1 Unlocking the past with newspaper data Imagine a historian, similar to that described in the Historian’s Macroscope, sits down to do some research. She is interested in the reception of news about the Crimean War, in Manchester. But where to start? Well, first she narrows down the list of newspapers to consult using an interactive map. For this research, she’ll look at everything published in Manchester between 1853 and 1857. But also, she’s interested in a more fine-grained group of titles: this will specifically be a study of the established press: so those that lasted a long time, at least fifty years in total. And while she’s at it, she’ll specify that she’s only interested in the Liberal press, using an enriched database of political leanings. List of titles in hand, she goes the BL Open Repository and finds the openly accessible ones, and downloads the relevant files, in .xml format. From here she extracts the text, article by article, into a single corpus. She makes a list of search terms which she thinks will help to narrow things down, and, using this, restricts her new corpus to articles about the Crimean war, as best as possible. First she looks at the most frequent terms: the usual suspects are there, of course - but once she filters out the ‘stop words’ she sees some potentially patterns, and notes down some words to dig into later. Giving the top ten per month is also interesting, and shows a shift from words relating to the diplomacy of the conflict, to more potentially more ‘emotive’ language, describing individual battles. Next she creates a 20 topic model to see if there is any difference between the types of articles in her corpus, which shows a few ‘themes’ under which the articles can be grouped together: one with words like steamer, navy, Sebastopol as its most important words is an unusual grouping, and might be worth exploring. Using sentiment analysis the historian of the Crimean war notices a shift towards reporting with more negative words, and a cluster of particularly negative articles in late 1854: when the reports of the failed military action during the Battle of Baklava started to trickle through: an event which was immortalised only weeks later in Tennyson’s narrative poem The Charge of the Light Brigade. Not surprising, perhaps, but a reassuring confirmation. How were these titles sharing information? Using techniques to find overlapping shared text across multiple documents, she works out that the flow of information moved from the established dailies to the weekly titles. This is not too far into the future: we’re starting to make data openly available. The tools, which only a few years ago were restricted to experts, are now unimaginably easier to use. Things have moved on from the first generation of ‘how-to’ guides for digital humanities students: it’s now fairly reasonable to pick a language, probably R or Python, and do all analysis, writing, documentation and so forth without ever leaving the safety of its ecosystem. These modern languages have a huge number of packages for doing all sorts of interesting analysis of text, and even authoring entire books. On the other hand, the promise of scraping the web for open data, while still with its place, has in many ways been superseded. The historian looking to use newspaper data must wrestle with closed systems, proprietary formats and so forth. The sheer quantity of newspaper data, and its commercial roots (and perhaps new commercial future), mean that it has not been treated in the same way as many other historical datasets. Newspaper data has, up until recently, had several financial, legal, bureaucratic and technical hurdles. 2.2 What can you do with newspaper data? There is a lot of newspaper data available now for historical researchers. Across the globe, the keepers of cultural memory are digitising their collections. Most news digitisation projects do OCR and zoning, meaning that the digitised images are processed so that the text is machine readable, and then divided into articles. It’s far from perfect - we’ll show some examples in a later chapter - but it does generate a large amount of data: both the digitised images, and the underlying text and information about the structure. Once you get hold of this data, the rewards can be huge: looking just at English-language users in the last few years, researchers have used it to understand Victorian jokes, trace the movement of information and ideas, understand the effects of industrialisation, track the meetings of radical groups, and of course understand more about the newspapers themselves and the print culture that surrounds them. While there has been a lot digitised, their is much, much more still to be done. The collection, in any country, is far from representative. But we must work with what we’ve got. The new histories of the press will be written by looking at text at scale, drawing broad conclusions, understanding genre, authorship and so forth through data analysis. We’re just at the beginning: in the last few years projects have been using neural networks methods to improve the kinds of things we can do: the Living with machines project, for example, or several projects at the Royal Library in the Netherlands. The methods I describe here are simplistic, but they can still add to our understanding. 2.3 Goals This short book hopes to help you: Know what British Library newspaper data is openly available, where it is, and where to look for more coming onstream. Understand something of the XML format which make up the Library’s current crop of openly available newspapers. Have a process for extracting the plain text of the newspaper in a format which is easy to use. Have been introduced to a number of tools which are particularly useful for large-scale text mining of huge corpora: n-gram counters, topic modelling, text re-use. Understand how the tools can be used to answer some basic historical questions (whether they provide answers, I’ll leave for the reader and historian to decide) 1 2.4 Why R? R is a programming language, much like Python. In the last few years it has become widely used by data scientists, those using digital humanities techniques, and social sciences. It has some advantages for a novice to programming: thanks to a very widely-used platform (called an Integrated Programming Environment or IDE) for the language called R-Studio, it is particularly easy to get up and running, writing code and drawing graphs, than many other languages. A lot of this is because of developers who have extended the functionality of the ‘base’ language greatly, particularly a suite of extra functions known collectively as the ‘tidyverse’. More on that in a bit. I think R comes into its own when its potential for mashups are realised: you could, for example, use sf draw a buffer around all of the UK’s waterways (or railways, or whatever), compare sentiment analysis scores in that newspapers from that buffer using tidytext, before connecting to wikidata and Open Street Map using their APIs and httr, and finally turn your analysis into an application using Shiny - all without leaving R-studio and a pretty common set of data standards. This is a handbook made with R, and using R, but it is not really primarily about R. It should be readable by anyone, but it’s possible bits will not be so easy to follow. I apologise in advance for any confusion or difficulties. If you’d like to get started with R and the tidyverse in earnest, I recommend some of these tutorials and books: http://dh-r.lincolnmullen.com https://blog.shotwell.ca/posts/why_i_use_r/ https://r4ds.had.co.nz 2.5 Getting started The only requirements to get through these tutorials are to install R and R-Studio, as well as some data which needs to be downloaded separately. 2.5.1 Download R and R-Studio R and R-Studio are two separate things. R will work without R-studio, but not the other way around, and so it should be downloaded first. It should be a simple process: Go to the download page here: https://www.r-project.org, select a download mirror, and download the correct version for your operating system. Follow the installation instructions here: https://cran.r-project.org/manuals.html if you get stuck, but it should be fairly painless. Next, download R-Studio here: https://rstudio.com/products/rstudio/. You’ll want the desktop version and again, download the correct version and follow the instructions. When both of these are installed, you just need to open R-Studio, and it should run the underlying software R automatically. At this point, I would highly recommend reading a beginners guide to R and R-studio, such as this one: https://moderndive.netlify.com/1-getting-started.html to familiarise yourself with the layout and some of the basic functionality of R-Studio. Once you understand where to type and save code, where your files and dataframes live, and how to import data from spreadsheets, you should be good to start experimenting with newspaper data. R relies on lots of additional packages for full functionality, and you’ll need to install these by using the function install.packages(), folled by the package name, in inverted commas. I recommend doing this to install the tidyverse suite of packages by running install.packages('tidyverse') in the Console window (the bottom-left of the screen in R-Studio) as you’ll end up using this all the time. 2.6 Who is the guide for? Historians looking to understand news at scale. Undergraduates and postgrads looking to dip their feet into computational methods. Imagine a PhD student, hasn’t used newspaper data before. What is available? How can she access it? Does she need to visit the library, or will she be happy with what’s available online? 2.7 Format of the book The book doesn’t need to be read in its entirety: if you’re just interested in finding newspaper data, specifically that collected by the British Library, you could stick with the first part. You might be a much better programmer than me and just interested in the scope of the Library’s datasets for some analysis - for instance, it might be useful to look at the bits on the BNA and JISC digitisation projects, to get a handle on how representative the data you have is. You might have arrived at the Library to do some research and not know where to begin with our folder structure and so forth. As much of that information as possible has been included. 2.8 Contribute The book has been written using Bookdown, which turns pages of code into figures and text, exposing the parts of the code when necessary, and hiding it in other cases. It lives on a GitHub repository, here: and the underlying code can be freely downloaded and re-created or altered. If you’d like to contribute, anything from a few corrections to an entire new chapter, please feel free to get in touch via the Github issues page or just fork the repository and request a merge when you’re done. Thomas T. Hills and others, ‘Historical Analysis of National Subjective Wellbeing Using Millions of Digitized Books’, Nature Human Behaviour, 3.12 (2019), 1271–5 &lt;https://doi.org/10.1038/s41562-019-0750-z&gt;.↩ "],
["uk-newspaper-data.html", "3 UK Newspaper Data 3.1 Intro to British Library Newspapers 3.2 Burney Collection 3.3 JISC Newspaper digitisation projects 3.4 British Newspaper Archive 3.5 British Library Openly available newspaper data 3.6 What access do you need?", " 3 UK Newspaper Data 3.1 Intro to British Library Newspapers The British Library holds about 60 million issues, or 450 million pages of newspapers. These cover over 400 years of British and world events, but the collection has not alway been systematic. As Ed King has written: Systematic collection of newspapers at the British Museum (the precursor of the British Library) did not really begin until 1822. At that time publishers were obliged to supply copies of their newspapers to the Stamp Office so that they could be taxed. In 1822 it was agreed that these copies would be passed to the British Museum after a period of three years. From 1869 onwards newspapers were included in the legal deposit legislation and were then deposited directly at the British Museum. This systematic application of legal deposit requirements means that many thousands of complete runs of newspapers have accumulated. The majority of newspapers collected are those published since 1800.2 What does this all mean for the data? First of all, it means that only a fraction of what was published has been preserved or collected, and only a fraction of that which has been collected has been digitised. Some very rough numbers: The ‘newspaper-year’ is a good standard unit. This is all the issues for one title, for one year. Its not perfect, because a newspaper-year of a weekly is worth the same as a newspaper-year for a daily. But it’s an easy unit to count. There’s currently about 40,000 newspaper-years on the British Newspaper Archive. The entire collection of British and Irish Newspapers is probably, at a guess, about 350,000 newspaper-years. It’s all very well being able to access the content, but for the purposes of the kind of things we’d like to do, access to the data is needed. The following are the main British Library digitised newspaper sources. Figure 3.1: An interactive map of the British Library’s physical newspaper collection. Links are to the catalogue entry. 3.2 Burney Collection The Burney Collection contains about one million pages, from the very earliest newspapers in the seventeenth century to the beginning of the 19th, collected by Rev. Charles Burney in the 18th century, and purchased by the Library in 1818.3 It’s actually a mixture of both Burney’s own collction and stuff inserted since. It was microfilmed in its entirety in the 1970s. As Andrew Prescott has written, ‘our use of the digital resource is still profoundly shaped by the technology and limitations of the microfilm set’4 The collection was imaged in the 90s but because of technological restrictions it wasn’t until 2007 when, with Gale, the British Library released the Burney Collection as a digital resource. The accuracy of the OCR has been measured, and one report found that the ocr for the Burney newspapers offered character accuracy of 75.6% and word accuracy of 65%.5. 3.3 JISC Newspaper digitisation projects Most of the projects in the UK which have used newspaper data have been using the British Library’s 19th Century Newspapers collection. This is an interesting collection of content and worth outlining in some detail. Knowing the sources, geographical makeup and motivation behind the titles in the collection can be really helpful in thinking about its representativeness. Figure 3.2: Very approximate chart of JISC titles, assuming that we had complete runs for all. Counted by year rather than number of pages digitised. The JISC newspaper digitisation program began in 2004, when The British Library received two million pounds from the Joint Information Systems Committee (JISC) to complete a newspaper digitisation project. A plan was made to digitise up to two million pages, across 49 titles.6 A second phase of the project digitised a further 22 titles.7 The titles cover England, Scotland, Wales and Ireland, and it should be noted that the latter is underrepresented although it was obviously an integral part of the United Kingdom at the time of the publication of these newspapers - something that’s often overlooked in projects using the JISC data. They cover about 40 cities ??, and are spread across 24 counties within Great Britain ??, plus Dublin and Belfast. The forty-eight titles chosen represent a very large cross-section of 19th century press and publishing history. Three principles guided the work of the selection panel: firstly, that newspapers from all over the UK would be represented in the database; in practice, this meant selecting a significant regional or city title, from a large number of potential candidate titles. Secondly, the whole of the nineteenth century would be covered; and thirdly, that, once a newspaper title was selected, all of the issues available at the British Library would be digitised. To maximise content, only the last timed edition was digitised. No variant editions were included. Thirdly, once a newspaper was selected, all of its run of issue would be digitised.8 Jane Shaw wrote, in 2007: The academic panel made their selection using the following eligibility criteria: To ensure that complete runs of newspapers are scanned To have the most complete date range, 1800-1900, covered by the titles selected To have the greatest UK-wide coverage as possible To include the specialist area of Chartism (many of which are short runs) To consider the coverage of the title: e.g., the London area; a large urban area (e.g., Birmingham); a larger regional/rural area To consider the numbers printed - a large circulation The paper was successful in its time via its sales To consider the different editions for dailies and weeklies and their importance for article inclusion or exclusion To consider special content, e.g., the newspaper espoused a certain political viewpoint (radical/conservative) The paper was influential via its editorials.9 What’s really clear, is that the selection was driven by assumed historical need by the Library’s users, plus some practicalities around copyright, microfilm and The result was a heavily curated collection, albeit with decent academic rigour and good intentions and like all collections created in this way, it is subject, quite rightly, to a lot of scrutiny by historians.10 This is all covered in lots of detail elsewhere, including some really interesting critiques of the access and so forth.11 But the overall makeup of it is clear, and this was a very specifically curated collection, though it was also influenced by contingency, in that it used microfilm (sometimes new microfilm). But overall, one might say that the collection has specific historical relevant, and was in ways representative. It does, though, only represent a tiny fraction of the newspaper collection, and by being relevant and restricted to ‘important’ titles, it does of course miss other voices. For example, much of the Library’s collection consists of short runs, and much of it has not been microfilmed, which means it won’t have been selected for digitisation. This means that 2019 digitisation selection policies are indirectly greatly influenced by microfilm selection policies of the 70s, 80s, and 90s. Subsequent digitisation projects are trying to rectify these motivations, but again, it’s good to keep in mind the ## Warning in validateCoords(lng, lat, funcName): Data contains 1 rows with either ## missing or invalid lat/lon values and will be ignored ## Warning in is.na(values): is.na() applied to non-(list or vector) of type ## &#39;closure&#39; Figure 3.3: Interactive map of the JISC Newspapers Currently researchers access this either through Gale, or through the British Library as an external researcher. Many researchers have requested access to the collection through Gale, which they will apparently do in exchange for a fee for the costs of the hard drives and presumably some labour time. The specifics of the XML used, and some code for extracting the data, are available in the following chapter. Some researchers have also got access to the collection through 3.4 British Newspaper Archive Most of the British Library’s digitised newspaper collection is available on the British Newspaper Archive (BNA). The BNA is a commercial product run by a family history company called FindMyPast. FindMyPast is responsible for digitising large amounts of the Library’s newspapers, mostly through microfilm. As such, they have a very different focus to the JISC digitisation projects. The BNA is constantly growing, and it already dwarfs the JISC projects by number of pages: the BNA currently hosts about 33 million pages, against the 3 million or so of the two JISC projects 3.4 There are several important implications for this. First, most data-driven historical work carried out on newspapers has used the JISC data, rather than the BNA collection, because of relative ease-of-access. It’s an important point, as there may be an assumption that this kind of work is generally In addition, as it is a contantly evolving dataset, reproducibility is difficult. There are some exceptions: the Bristol N-Gram and named entity datasets used the FMP data, processing the top phrases and words from about 16 million pages. The collection has doubled in size since then: it’s likely that were it to be run again the results would be different. This is not only because of volume but also because of the change in focus and digitisation policy. Newspapers are selected for various reasons, but an underlying principle of coverage seems to be important: newspapers are obviously not selected at random, which necessarily results in a changing, evolving idea of bias. Figure 3.4: Newspaper-years on the British Newspaper Archive. Note that this includes JISC content above. Figure 3.5: Titles on the British Newspaper Archive.12 3.5 British Library Openly available newspaper data Currently, the Library makes data from the Heritage Made Digital digitisation project available. This is a project within the British Library to digitise up to 1.3 million pages of 19th century newspapers. It has a specific curatorial focus: it picked titles which are completely out of copyright, which means that they all finished publication before 1879. It also had preservation aims: because of this, it chose titles which were not on microfilm, and were also in poor or unfit condition. Newspaper volumes in unfit condition cannot be called up by readers: this meant that if a volume is not microfilmed and is in this state, it can’t be read by anyone. There’s some more about the project available here: https://blogs.bl.uk/thenewsroom/2019/01/heritage-made-digital-the-newspapers.html The other curatorial goal was to focus on ‘national’ titles. In practice this meant choosing titles printed in London, but without a specific London focus. The JISC digitisation projects focused on regional titles, then local, and all the ‘big’ nationals like the Times or the Manchester Guardian have been digitised by their current owners. This means that a bunch of historically important titles may have fallen through the cracks, and this projects is digitising some of those.13 The good news is that as these newspapers are deemed out of copyright, the data can be made freely downloadable. Currently the first batch of newspapers, in METS/ALTO format, are available on the British Library’s Open Repository. They have a CC-0 licence, which means they can be used for any purpose whatsoever. The code examples in the following chapters will use this data. 3.6 What access do you need? 3.6.1 You want to find individual articles? Access through the BNA or Gale data, depending on access. BNA if you need the most coverage, and are interested particularly in local or regional titles. Gale has better search facilities but less content. ### You want to do some simple text mining, not so bothered about regional coverage Probably access HMD titles through the repository 3.6.2 You want to do text mining on a large corpus Request access to the Library’s newspapers through BL labs or elsewhere. Get access to raw files. Do you own analysis or else follow steps here to extract the data. There’s instructions for JISC and HMD titles. 3.6.3 You want to do text mining on the entire digitised collection You’ll need to speak to FMP, who run the British Newspaper Archive. They do take requests for access. There is a dataset of n-grams which has been used for text mining, sentiment analysis etc. and might be useful, though the collection has grown significantly since this point. It’s available as a free download from here: https://doi.org/10.5523/bris.dobuvuu00mh51q773bo8ybkdz 3.6.4 You want to do something involving the images, such as computer vision techniques, You’ll probably need to request access to newspapers through the British Library, or through Gale. Gale will send a copy of the JISC 1 &amp; 2 data (with OCR enhancements) on a hard drive to researchers, for a fee. Access through the Library will allow for image analysis but might be difficult to take away. The images up to 1878 are cleared for reuse. Ed King, ‘Digitisation of British Newspapers 1800-1900’, 2007 &lt;https://www.gale.com/intl/essays/ed-king-digitisation-of-british-newspapers-1800-1900&gt; [accessed 2007].↩ Andrew Prescott, ‘Travelling Chronicles: News and Newspapers from the Early Modern Period to the Eighteenth Century’, in, ed. by Siv Gøril Brandtzæg, Paul Goring, and Christine Watson (Leiden, The Netherlands: Brill, 2018), pp. 51–71.↩ Prescott.↩ Prescott.↩ King.↩ Jane Shaw, ‘Selection of Newspapers’, British Library Newspapers, 2007 &lt;https://www.gale.com/intl/essays/jane-shaw-selection-of-newspapers&gt;; Jane Shaw, ‘10 Billion Words: The British Library British Newspapers 1800-1900 Project: Some Guidelines for Large-Scale Newspaper Digitisation’, 2005 &lt;https://archive.ifla.org/IV/ifla71/papers/154e-Shaw.pdf&gt; for a good brief overview to the selection process for JISC 1.↩ Ed King, ‘British Library Digitisation: Access and Copyright’, 2008.↩ Shaw.↩ Paul Fyfe, ‘An Archaeology of Victorian Newspapers’, Victorian Periodicals Review, 49.4 (2016), 546–77 &lt;https://doi.org/10.1353/vpr.2016.0039&gt; for example.↩ Thomas Smits, ‘Making the News National: Using Digitized Newspapers to Study the Distribution of the Queen’s Speech by W. H. Smith &amp; Son, 1846–1858’, Victorian Periodicals Review, 49.4 (2016), 598–625 &lt;https://doi.org/10.1353/vpr.2016.0041&gt;; James Mussell, ‘Elemental Forms: Elemental Forms: The Newspaper as Popular Genre in the Nineteenth Century’, Media History, 20.1 (2014), 4–20 &lt;https://doi.org/10.1080/13688804.2014.880264&gt; both include some discussion and critique of the British Library Newspaper Collection.↩ data from https://www.britishnewspaperarchive.co.uk/titles/↩ The term national is debatable, but it’s been used to try and distinguish from titles which clearly had a focus on one region. Even this is difficult: regionals would have often had a national focus, and were in any case reprinting many national stories. But their audience would have been primarily in a limited geographical area, unlike a bunch of London-based titles, which were printed and sent out across the country, first by train, then the stories themselves by telegraph.↩ "],
["historic-newspaper-ocr-accuracy.html", "4 Historic Newspaper OCR Accuracy 4.1 Introduction 4.2 What is it like in 19th century newspapers? 4.3 OCR report on some batches of historical newspapers 4.4 Extract predicted word scores from the ALTO pages 4.5 Visualisations: 4.6 Highest and lowest results: 4.7 Page-by-page OCR visualisation 4.8 Microfilm vs print: 4.9 Conclusions 4.10 Impact on analysis", " 4 Historic Newspaper OCR Accuracy 4.1 Introduction OCR, or ‘Optical Character Recognition’, is a series of methods for turning the text in digitised images into machine-readable code. 4.2 What is it like in 19th century newspapers? This is a difficult question to answer, because it varies so much between projects, format and dates. The truth is, nobody really knows what it’s like, because that would involve having large sets of very accurate, manually transcribed newspapers, to compare to the OCR text. Subjectively, we can probably make a few generalisations. It gets better as the software gets better, but not particularly quickly, because much of the quality is dependant on things to do with the physical form. Digitising from print is much better than from microfilm. But print can still be bad. Standard text is much better than non-standard. For example, different fonts, sizes, and so forth. Advertisements seem to have particularly bad OCR - they are generally not in regular blocks of text, which the OCR software finds difficult, and they often used non-standard characters or fonts to stand out. The time dimension is not clear: type probably got better, but it also got smaller, more columns. Problems with the physical page have a huge effect: rips, tears, foxing, dark patches and so forth. Many errors are not because of the microfilm, digital image or software, and may not be fixable. What does this all mean? Well, it introduces bias, and probably in non-random ways, but in ways that have implications for our work. If things are digitised from a mix of print and microfilm, for example, we might get very different results for the print portion, which might easily be mis-attributed to a significant historical finding.14 4.3 OCR report on some batches of historical newspapers The files returned from newspaper digitisers contain a ‘predicted word accuracy score’ percentage for each page. These can be extracted and visualised, with some interesting conclusions. However it’s important to note these are not calculated by comparing actual results to the OCR, but rather use an internal algorithm. Some links worth reading to understand more about OCR and confidence scores: OCR software calculates a confidence level for each character it detects. Word and page confidence levels can be calculated from the character confidences using algorithms either inside the OCR software or as an external customised process. The OCR software doesn’t know whether any character is correct or not – it can only be confident or not confident that it is correct. It will give it a confidence level from 0-9. True accuracy, i.e., whether a character is actually correct, can only be determined by an independent arbiter, a human. This can be done by proofreading articles or pages, or by manually re-keying the entire article or page and comparing the output to the OCR output. These methods are very time consuming. (http://www.dlib.org/dlib/march09/holley/03holley.html) Because Abbyy Finereader is a commercial product, the software that predicts its accuracy is not freely available for inspection. As such, we should not make too much of the figure presented here, which certainly does not align with a human reader’s assessment of the page’s overall similarity to the words on the page images. (https://ryancordell.org/research/qijtb-the-raven-mla/) 4.4 Extract predicted word scores from the ALTO pages Generate Library colour scheme palettes: 4.5 Visualisations: 4.5.1 What’s in this data? The data includes 290,000 separate ALTO files, each representing one page. From the files, the ‘predicted word accuracy’ score has been extracted, and turned into a dataframe. The data contains about 117,000 files digitised from microfilm, and 173,000 digitised from print. This makes an interesting dataset to compare OCR quality scores across two different formats, by the same company at the same time. Comparison between pages: This visualisation shows pages on the y axis and time on the x axis. Each page is a separate ‘band’. Lighter colours (yellow) represent a higher reported score. Front pages have consistently lower scores than other pages. This is mostly because the front pages of 19th century newspapers contained mostly adverts, which OCR software finds difficult to process because of the variety in type and layout. This visualisation also shows the existence of multiple editions: dark lines on pages 9, 17 etc. are front pages of subsequent editions which have also been scanned under the same date. Points have been randomly spaced out for readability. Figure 4.1: OCR accuracy visualised by page, across the dataset. Lighter colours represent higher accuracy. Clear difference between the front and subsequent pages can be seen. 4.6 Highest and lowest results: The lowest results are all from the Lady’s Newspaper - this was an illustrated title and so the score is probably meaningless. accuracy nlp date page 15.8 0002254 1859-09-03 0005 16.5 0002254 1859-12-31 0019 16.5 0002254 1860-01-07 0019 16.5 0002254 1860-07-21 0013 16.5 0002254 1862-01-25 0013 16.5 0002254 1862-04-19 0013 16.6 0002254 1859-10-22 0019 16.7 0002254 1859-11-12 0012 16.7 0002254 1860-01-21 0019 16.7 0002254 1860-09-08 0012 The highest scores are blank pages: accuracy nlp date page 100 0002083 1853-10-15 0006 100 0002083 1853-10-26 0010 100 0002083 1853-11-09 0010 100 0002083 1853-11-14 0010 100 0002083 1853-11-15 0010 100 0002083 1853-11-16 0010 100 0002083 1853-11-17 0010 100 0002083 1853-11-19 0010 100 0002083 1853-11-21 0010 100 0002083 1853-11-23 0010 4.7 Page-by-page OCR visualisation We can look at the difference in OCR accuracy by page position. The front page consistently has the lowest predicted accuracy. The exception is a group of first-page files in late 1860s: these were copies of the Sun and Central Press which were printed with two columns and large type, and without adverts on the first page. In general the predicted accuracy scores move upwards over time, and variation decreases. This is particularly clear in titles processed from print as the ntext Figure 4.2: Visualising OCR accuracy scores. Each dot represents a single page, positioned by date and reported accuracy. Pages are coloured by page position. Only the first four page positions are shown, for readability 4.8 Microfilm vs print: Approximately half of the data is from titles which were processed from microfilm, allowing a useful comparison between the scores of microfilm and print titles. The microfilm titles have, as expected, consistently lower accuracy, particularly the distribution. Particularly apparent is the difference in improvement over time: There’s no obvious increase in the scores of microfilm titles over time, but there is a significant change in print titles: from 1825 the predicted accuracy scores for print increase significantly, and the variation reduces noticeably. Figure 4.3: Microfilm vs Print: difference in the distribution and evolution of accuracy scores for titles digitised from both formats. Charting the average score (averaged over an entire year, so take with a pinch of salt) shows the different between microfilm and print more starkly: Figure 4.4: A broad view of improvement. Print titles show much more improvement in the assessed accuracy of the OCR over time 4.9 Conclusions This short report shows that the OCR accuracy -if the predicted word accuracy score included in the ALTO metadata is in any way a useful proxy - improves over time, and from 1825 onwards, the predicted scores for titles scanned from print are particularly high and consistent. Pages of advertising, as expected, show the lowest accuracy scores, and the scores are meaningless for illustrated titles. These reports could be generated for each batch going forward, and made available to researchers using the OCR for research. 4.10 Impact on analysis It depends. Broad analysis still seems to work - keyword searches, for example, come up with broadly expected results. It might be more important in finer work, for example Natural Language Processing (NLP). NLP relies on Why You (A Humanist) Should Care About Optical Character Recognition Mark John Hill and Simon Hengchen, ‘Quantifying the Impact of Dirty Ocr on Historical Text Analysis: Eighteenth Century Collections Online as a Case Study’, Digital Scholarship in the Humanities : DSH, 2019 &lt;https://doi.org/10.1093/llc/fqz024&gt;, @Cordell_2017, @Piotrowski_2012, @cordell-ocr, @evershed-ocr.↩ "],
["quick-introduction-to-r-and-the-tidyverse.html", "5 Quick introduction to R and the tidyverse 5.1 What is R and why should you use it? 5.2 Using R 5.3 Tidyverse", " 5 Quick introduction to R and the tidyverse The motivation behind this book was to provide a way to access and analyse newspaper data using a programming language that I am familiar with. The reason is simple: you write what you know, and I know R best. With its interface, R-Studio, I think it has the easiest transition from someone used to spreadsheet programs, and you’ll realise that most of what you do is filter, sort, count and select columns in a data format called a dataframe. 5.1 What is R and why should you use it? I think R and R-Studio have the easiest transition from someone used to Excel, and you’ll realise that most of what you do is filter, sort, count and select columns in a data format called a dataframe. A dataframe is basically a spreadsheet - it contains rows with observations, and columns with variables. Each row is generally a thing, for want of a better word. A thing that wants to be counted, either by summarising it as a more general thing, or turning it into something else and then counting it, or removing some of the things first and then counting the leftovers. For me, thing might be a record of a newspaper title, or a newspaper article (and its text), or it might be a single word. You can do a lot more interesting tasks with a thing in a dataframe. A thing might be a single polygon, in a huge dataframe of polygons or lines, all of which add up to a map, which we can then count, sort, filter and render as an image or even an interactive. 5.2 Using R 5.2.1 Base R commands I don’t use them very much, but R does have a bunch of very well-developed commands for doing the sorting, filtering and counting mentioned above. If you want to learn base R, I recommend the following: It is worth understanding the main types of data that you’ll come across, in your environment window. First, you’ll have dataframes. These are the spreadsheet-like objects which you’ll use in most analyses. They have rows and columns. Next are variables. A variable is assigned to a name, and then used for various purposes. You’ll often hear of an item called a vector. A vector is like a python list, if that means anything to you. A vector can be a single column in a dataframe (spreadsheet), which means they are used very often in R to manipulate data. A vector can have different types: for example, a character vector looks like this c(&quot;apples&quot;, &quot;bananas&quot;, &quot;oranges&quot;) A dataframe is just a bunch of vectors side by side. A vector is created with the command c(), with each item in the vector placed between the brackets, and followed by a comma. If your vector is a vector of words, the words need to be in inverted commas or quotation marks. fruit = c(&quot;apples&quot;, &quot;bananas&quot;, &quot;oranges&quot;, &quot;apples&quot;) colour = c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;) amount = c(2,5,10,8) You can create a dataframe using the data.frame() command. You just need to pass the function each of your vectors, which will become your columns. fruit_data = data.frame(fruit,colour,amount, stringsAsFactors = FALSE) Notice above that the third column, the amount, has under it instead of . That’s because R is treating it as a number, rather than a character. This means you can add them up and do all sorts of other mathy type things to them. All the items in a vector are coerced to the same type. So if you try to make a vector with a combination of numbers and strings, the numbers will be converted to strings. I wouldn’t worried too much about that for now. So for example if you create this vector, the numbers will get converted into strings. fruit = c(&quot;apples&quot;, 5, &quot;oranges&quot;, 3) fruit ## [1] &quot;apples&quot; &quot;5&quot; &quot;oranges&quot; &quot;3&quot; Anyway, that’s a dataframe. 5.3 Tidyverse Most of the work in these notebooks is done using a set of packages developed for R called the ‘tidyverse’. These enhance and improve a large range of R functions, with much nice syntax - and they’re faster too. It’s really a bunch of individual packages for sorting, filtering and plotting data frames. They can be divided into a number of diferent categories. All these functions work in the same way. The first argument is the thing you want to operate on. This is nearly always a data frame. After come other arguments, which are often specific columns, or certain variables you want to do something with. You installed the package in the last notebook. Make sure the library is loaded by running the following in an R chunk in a notebook: library(tidyverse) Here are a couple of the most important ones 5.3.1 select(), pull() select() allows you to select columns. You can use names or numbers to pick the columns, and you can use a - sign to select everything but a given column. Using the fruit data frame we created above: We can select just the fruit and colour columns: select(fruit_data, fruit, colour) ## fruit colour ## 1 apples green ## 2 bananas yellow ## 3 oranges orange ## 4 apples red Select everything but the colour column: select(fruit_data, -colour) ## fruit amount ## 1 apples 2 ## 2 bananas 5 ## 3 oranges 10 ## 4 apples 8 Select the first two columns: select(fruit_data, 1:2) ## fruit colour ## 1 apples green ## 2 bananas yellow ## 3 oranges orange ## 4 apples red 5.3.2 group_by(), tally(), summarise() The next group of functions group things together and count them. Sounds boring but you would be amazed by how much of data science just seems to be doing those two things in various combinations. group_by() puts rows with the same value in a column of your dataframe into a group. Once they’re in a group, you can count them or summarise them by another variable. First you need to create a new dataframe with the grouped fruit. grouped_fruit = group_by(fruit_data, fruit) Next we use tally(). This counts all the instances of each fruit group. tally(grouped_fruit) ## # A tibble: 3 x 2 ## fruit n ## &lt;chr&gt; &lt;int&gt; ## 1 apples 2 ## 2 bananas 1 ## 3 oranges 1 See? Now the apples are grouped together rather than being two separate rows, and there’s a new column called n, which contains the result of the count. If we specify that we want to count by something else, we can add that in as a ‘weight’, by adding wt = as an argument in the function. tally(grouped_fruit, wt = amount) ## # A tibble: 3 x 2 ## fruit n ## &lt;chr&gt; &lt;dbl&gt; ## 1 apples 10 ## 2 bananas 5 ## 3 oranges 10 That counts the amounts of each fruit, ignoring the colour. 5.3.3 filter() Another quite obviously useful function. This filters the dataframe based on a condition which you set within the function. The first argument is the data to be filtered. The second is a condition (or multiple condition). The function will return every row where that condition is true. Just red fruit: filter(fruit_data, colour == &#39;red&#39;) ## fruit colour amount ## 1 apples red 8 Just fruit with at least 5 pieces: filter(fruit_data, amount &gt;=5) ## fruit colour amount ## 1 bananas yellow 5 ## 2 oranges orange 10 ## 3 apples red 8 5.3.4 sort(), arrange(), top_n() Another useful set of functions, often you want to sort things. The function arrange() does this very nicely. You specify the data frame, and the variable you would like to sort by. arrange(fruit_data, amount) ## fruit colour amount ## 1 apples green 2 ## 2 bananas yellow 5 ## 3 apples red 8 ## 4 oranges orange 10 Sorting is ascending by default, but you can specify descending using desc(): arrange(fruit_data, desc(amount)) ## fruit colour amount ## 1 oranges orange 10 ## 2 apples red 8 ## 3 bananas yellow 5 ## 4 apples green 2 If you `sortarrange() by a list of characters, you’ll get alphabetical order: arrange(fruit_data, fruit) ## fruit colour amount ## 1 apples green 2 ## 2 apples red 8 ## 3 bananas yellow 5 ## 4 oranges orange 10 You can sort by multiple things: arrange(fruit_data, fruit, desc(amount)) ## fruit colour amount ## 1 apples red 8 ## 2 apples green 2 ## 3 bananas yellow 5 ## 4 oranges orange 10 Notice that now red apples are first. 5.3.5 left_join(), inner_join(), anti_join() 5.3.6 Piping Another great feature of the tidyverse is that you can ‘pipe’ commands through a bunch of functions. This means that you can do one operate, and pass the result to another operation. The previous dataframe is passed as the first argument of the next function by using the pipe %&gt;% command. It works like this: fruit_data %&gt;% filter(colour != &#39;yellow&#39;) %&gt;% # remove any yellow colour fruit group_by(fruit) %&gt;% # group the fruit by type tally(amount) %&gt;% # count each group arrange(desc(n)) # arrange in descending order of the count ## # A tibble: 2 x 2 ## fruit n ## &lt;chr&gt; &lt;dbl&gt; ## 1 apples 10 ## 2 oranges 10 That code block, written in prose: “take fruit data, remove any yellow colour fruit, count the fruits by type and amount, and arrange in descending order of the total” 5.3.7 Plotting using ggplot() The tidyverse includes a pretty great plotting library called ggplot2. This can be used by piping your dataframe to a function called ggplot(). The basic idea is that you add your data, then you can add plot elements which are called geoms. Some common ones are geom_line(), geom_bar() and geom_point(). To the geom function you add aesthetics, which is basically telling the function which bits of your data should be responsible for which parts of the visualisation. These are added using aes(). I’ll explain a bit more about some of these aesthetics as I go along. As an example: Bar chart of different types of fruit (one each of bananas and oranges, two types of apple) fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit)) Figure 5.1: fruit Counting the total amount of fruit: fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit, weight = amount)) Figure 5.2: totals Charting amounts and fruit colours: fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit, weight = amount, fill = colour)) Figure 5.3: grouped fruit And just because it annoys me having random colours, we can map them to the actual colours: fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit, weight = amount, fill = colour)) + scale_fill_manual(values = c(&quot;orange&quot; = &quot;orange&quot;, &quot;green&quot; = &quot;#8db600&quot;, &quot;red&quot; = &quot;#ff0800&quot;, &quot;yellow&quot; = &quot;#ffe135&quot;)) Figure 5.4: Fruit chart with nice colours 5.3.8 Doing this with newspaper data Who cares about fruit? Nobody, that’s who. We want newspaper data! Let’s load a dataset of metadata for all the titles held by the library, and do some counting and sorting. Download from here: British Library Research Repository You would need to extract into your project folder first, if you’re following along: read_csv reads the csv from file. title_list = read_csv(&#39;data/BritishAndIrishNewspapersTitleList_20191118.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## title_id = col_double(), ## nid = col_double(), ## nlp = col_double(), ## first_date_held = col_double(), ## publication_date_one = col_double(), ## publication_date_two = col_double() ## ) ## See spec(...) for full column specifications. Select some particularly relevant columns: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) ## # A tibble: 24,927 x 4 ## publication_title first_date_held last_date_held country_of_publi… ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Corante, or, Newes from It… 1621 1621 The Netherlands ## 2 &quot;Corante, or, Newes from It… 1621 1621 The Netherlands ## 3 &quot;Corante, or, Newes from It… 1621 1621 The Netherlands ## 4 &quot;Corante, or, Newes from It… 1621 1621 England ## 5 &quot;Courant Newes out of Italy… 1621 1621 The Netherlands ## 6 &quot;A Relation of the late Occ… 1622 1622 England ## 7 &quot;A Relation of the late Occ… 1622 1622 England ## 8 &quot;A Relation of the late Occ… 1622 1622 England ## 9 &quot;A Relation of the late Occ… 1622 1622 England ## 10 &quot;A Relation of the late Occ… 1622 1622 England ## # … with 24,917 more rows Arrange in order of the latest date of publication, and then by the first date of publication: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held), first_date_held) ## # A tibble: 24,927 x 4 ## publication_title first_date_held last_date_held country_of_publi… ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Shrewsbury chronicle 1773 Continuing England ## 2 London times|The Times|Time… 1788 Continuing England ## 3 Observer (London)|Observer … 1791 Continuing England ## 4 Limerick chronicle 1800 Continuing Ireland ## 5 Hampshire chronicle|The Ham… 1816 Continuing England ## 6 The Inverness Courier, and … 1817 Continuing Scotland ## 7 Sunday times (London)|Sunda… 1822 Continuing England ## 8 The Impartial Reporter, etc 1825 Continuing Northern Ireland ## 9 Impartial reporter and farm… 1825 Continuing Northern Ireland ## 10 Aberdeen observer 1829 Continuing Scotland ## # … with 24,917 more rows Group and count by country of publication: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() ## # A tibble: 40 x 2 ## country_of_publication n ## &lt;chr&gt; &lt;int&gt; ## 1 Bermuda Islands 24 ## 2 Cayman Islands 1 ## 3 England 20465 ## 4 England|Hong Kong 1 ## 5 England|India 2 ## 6 England|Iran 2 ## 7 England|Ireland 10 ## 8 England|Ireland|Northern Ireland 10 ## 9 England|Jamaica 7 ## 10 England|Malta 2 ## # … with 30 more rows Arrange again, this time in descending order of number of titles for each country: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() %&gt;% arrange(desc(n)) ## # A tibble: 40 x 2 ## country_of_publication n ## &lt;chr&gt; &lt;int&gt; ## 1 England 20465 ## 2 Scotland 1778 ## 3 Ireland 1050 ## 4 Wales 1019 ## 5 Northern Ireland 415 ## 6 England|Wales 58 ## 7 Bermuda Islands 24 ## 8 England|Scotland 13 ## 9 England|Ireland 10 ## 10 England|Ireland|Northern Ireland 10 ## # … with 30 more rows Filter only those with more than 100 titles: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% filter(n&gt;=100) ## # A tibble: 5 x 2 ## country_of_publication n ## &lt;chr&gt; &lt;int&gt; ## 1 England 20465 ## 2 Scotland 1778 ## 3 Ireland 1050 ## 4 Wales 1019 ## 5 Northern Ireland 415 Make a simple bar chart: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% filter(n&gt;=100) %&gt;% ggplot() + geom_bar(aes(x = country_of_publication, weight = n)) Figure 5.5: barchart So that’s a very quick introduction to R. There’s loads of places to learn more. R-studio cheat sheets The Pirate’s Guide to R, a good beginners guide to base R R for data science, which teaches the tidyverse in detail Learn how to make a book like this using Bookdown "],
["mapping-with-r-geocode-and-map-the-british-librarys-newspaper-collection.html", "6 Mapping with R: Geocode and Map the British Library’s Newspaper Collection 6.1 Mapping with ggplot2 and mapdata 6.2 Drawing a background map. ` 6.3 Add Sized Points By Coordinates 6.4 Bringing it all together 6.5 Drawing a newspaper titles ‘Choropleth’ map with R and the sf package 6.6 Get county information from the title list 6.7 Make the points object using geom_sf() 6.8 Transform from UTM to lat/long using st_transform() 6.9 Download and merge the title list with a set of coordinates. 6.10 Using st_join to connect the title list to the shapefile 6.11 Draw using ggplot2 and geom_sf()", " 6 Mapping with R: Geocode and Map the British Library’s Newspaper Collection R is great for making maps, whether for visualising historical material or doing more advanced analysis. Using an openly available list of British Library titles, with some additional coordinate information, it’s possible to very quickly make high-quality, publishable maps. 6.1 Mapping with ggplot2 and mapdata 6.1.1 A map of British Newspapers by City With R and ggplot2, it’s possible to quickly map and understand the geographic coverage of the newspapers held by the British Library. To do this we’ll need three elements: A background map of the UK and Ireland A count of the total titles for each city A list of coordinates for each place mentioned in the title list. 6.2 Drawing a background map. ` The first thing needed, is a background back of the UK, to which points, sized by number of titles, will be added. The plotting library ggplot2, which is part of the tidyverse package, contains a function called map_data() which turns data from the maps library into a dataframe. The usual functions of ggplot2 can then be used to draw a map. First you’ll need to install the maps package using install.packages(). This package contains the actual data which can then be called with the ggplot2 function map_data(). Next load ggplot2 and the maps library library(ggplot2) library(maps) First create a dataframe called ‘worldmap’ with a function called map_data(). map_data() takes an argument with the name of the map you want to load, in inverted commas. Some of the choices are ‘world’, ‘usa’, ‘france’, ‘italy’. We’ll use the ‘world’ map. worldmap = map_data(&#39;world&#39;) Take a look at the dataframe we’ve created: knitr::kable(head(worldmap, 20)) long lat group order region subregion 1 -69.89912 12.45200 1 1 Aruba NA 2 -69.89571 12.42300 1 2 Aruba NA 3 -69.94219 12.43853 1 3 Aruba NA 4 -70.00415 12.50049 1 4 Aruba NA 5 -70.06612 12.54697 1 5 Aruba NA 6 -70.05088 12.59707 1 6 Aruba NA 7 -70.03511 12.61411 1 7 Aruba NA 8 -69.97314 12.56763 1 8 Aruba NA 9 -69.91181 12.48047 1 9 Aruba NA 10 -69.89912 12.45200 1 10 Aruba NA 12 74.89131 37.23164 2 12 Afghanistan NA 13 74.84023 37.22505 2 13 Afghanistan NA 14 74.76738 37.24917 2 14 Afghanistan NA 15 74.73896 37.28564 2 15 Afghanistan NA 16 74.72666 37.29072 2 16 Afghanistan NA 17 74.66895 37.26670 2 17 Afghanistan NA 18 74.55899 37.23662 2 18 Afghanistan NA 19 74.37217 37.15771 2 19 Afghanistan NA 20 74.37617 37.13735 2 20 Afghanistan NA 21 74.49796 37.05722 2 21 Afghanistan NA It’s a big table with about 100,000 rows. Each row has a latitude and longitude, and a group. Each region and sub-region in the dataframe has its own group number. We’ll use a function geom_polygon which tells ggplot to draw a polygon (a bunch of connected lines) for each group, and display it. With the aes(), x tells ggplot2 the longitude of each point, y the latitude, and group makes sure the polygons are grouped together correctly. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group)) Right, it needs a bit of tweaking. First, we only want to plot points in the UK. There’s obviously way too much map for this, so the first thing we should do is restrict it to a rectangle which includes those two countries. We can do that with coord_fixed(). coord_fixed() is used to fix the aspect ratio of a coordinate system, but can be used to specify a bounding box by using two of its arguments: xlim= and ylim=. These each take a vector (a series of numbers) with two items A vector is created using c(). Each item in the vector specifies the limits for that axis. So xlim = c(0,10) means restrict the x-axis to 0 and 10. The axes correspond to the lines of longitude (x) and latitude (y). We’ll restrict the x-axis to c(-10, 4) and the y-axis to c(50.3, 60) which should just about cover the UK and Ireland. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group)) + coord_fixed(xlim = c(-10,3), ylim = c(50.3, 59)) Figure 6.1: Empty Map You can also change the aspect ratio of the coordinates using another ggplot function, coord_fixed(). The default is 1, but by specifying a different one with the argument ratio =, that can be changed. Using ratio = 1.3 results in a less squashed-looking map. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group)) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) Figure 6.2: Aspect Ratio Adjustment A couple more things, which I’ll run through quickly. You can specify fill and line colors usings fill = and color = inside geom_polygon() but outside aes(). ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) Figure 6.3: Colours and Fill We probably don’t need the grids or panels in the background. We can get rid of these with + theme_void(). ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) + theme_void() 6.3 Add Sized Points By Coordinates 6.3.1 Get a count of the total titles for each city This next bit uses some of the functions demonstrated in the introduction to R and the tidyverse, namely group_by() and tally(). First load the rest of the tidyverse packages. library(tidyverse) Next, load the title list, which can be dowloaded from the British Library’s Open Repository title_list = read_csv(&#39;data/BritishAndIrishNewspapersTitleList_20191118.csv&#39;) We can quite easily make a new data frame, which will just include each location and the total number of instances in the dataset. location_counts = title_list %&gt;% group_by(country_of_publication, general_area_of_coverage, coverage_city) %&gt;% tally() Arranging these in descending order of their count shows how many of each we have: knitr::kable(location_counts %&gt;% arrange(desc(n)) %&gt;% head(10)) country_of_publication general_area_of_coverage coverage_city n England London London 5781 Ireland Dublin (Ireland : County) Dublin 415 Scotland Strathclyde Glasgow 309 England Greater Manchester Manchester 265 England West Midlands Birmingham 260 England Merseyside Liverpool 220 England Avon Bristol 175 Scotland Lothian Edinburgh 162 England South Yorkshire Sheffield 133 England Nottinghamshire Nottingham 127 6.3.2 Get hold of a list of geocoordinates These coordinates have been produced in cooperation with another project with the Library, Living with Machines. We used smart annotations to quickly correct and train a georeferencer, to generate coordinates with much higher accuracy than, say, geonames or a google georeferencer. These can be joined to the title list downloaded from the repository, and then easily mapped. geocorrected = read_csv(&#39;data/geocorrected.csv&#39;) Change the column names: library(snakecase) colnames(geocorrected) = to_snake_case(colnames(geocorrected)) Do a bit of pre-processing to this. Change some column names further, select just the relevant columns, change the NA values and get rid of any empty entries. colnames(geocorrected)[6:8] = c(&#39;wikititle&#39;, &#39;lat&#39;, &#39;lng&#39;) geocorrected = geocorrected %&gt;% select(-1, -9,-10, -11, -12) geocorrected = geocorrected %&gt;% mutate(country_of_publication = replace(country_of_publication, country_of_publication == &#39;na&#39;, NA)) %&gt;% mutate(general_area_of_coverage = replace(general_area_of_coverage, general_area_of_coverage == &#39;na&#39;, NA)) %&gt;% mutate(coverage_city = replace(coverage_city, coverage_city == &#39;na&#39;, NA)) geocorrected = geocorrected %&gt;% mutate(lat = as.numeric(lat)) %&gt;% mutate(lng = as.numeric(lng)) %&gt;% filter(!is.na(lat)) %&gt;% filter(!is.na(lng)) The result is a dataframe with a set of longitude and latitude points (they come from Wikipedia, which is why they are prefixed with wiki) for every combination of city/county/country in the list of titles. These can be joined to the full title list with the following method: Using left_join() we will merge these dataframes, joining up each set of location information to its coordinates and standardised name. left_join() is a very common command in data analysis. It merges two sets of data by matching a value known as a key. Here the key is three values - city, county and country, and it matches up the two sets of data by ‘joining’ two rows together, if they share all three of these values. Store this is a new variable called lc_with_geo. lc_with_geo = location_counts %&gt;% left_join(geocorrected, by = c(&#39;coverage_city&#39; , &#39;general_area_of_coverage&#39;, &#39;country_of_publication&#39;)) If you look at this new dataset, you’ll see that now the counts of locations have merged with the geocorrected data. Now we have an amount and coordinates for each place. head(lc_with_geo, 10) ## # A tibble: 10 x 8 ## # Groups: country_of_publication, general_area_of_coverage [9] ## country_of_publ… general_area_of… coverage_city n places wikititle lat ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bermuda Islands &lt;NA&gt; Hamilton 23 &lt;NA&gt; &lt;NA&gt; NA ## 2 Bermuda Islands &lt;NA&gt; Saint George 1 &lt;NA&gt; &lt;NA&gt; NA ## 3 Cayman Islands &lt;NA&gt; Georgetown 1 &lt;NA&gt; &lt;NA&gt; NA ## 4 England Aberdeenshire (… Peterhead 1 &lt;NA&gt; &lt;NA&gt; NA ## 5 England Acton (London, … Ealing (Lond… 1 &lt;NA&gt; &lt;NA&gt; NA ## 6 England Aintree |Merse… Maghull 3 &lt;NA&gt; &lt;NA&gt; NA ## 7 England Alford (Lincoln… Mablethorpe 1 &lt;NA&gt; &lt;NA&gt; NA ## 8 England Alford (Lincoln… Skegness 1 &lt;NA&gt; &lt;NA&gt; NA ## 9 England Alfriston |Eas… Newhaven 1 &lt;NA&gt; &lt;NA&gt; NA ## 10 England Altrincham |Gr… Sale 2 &lt;NA&gt; &lt;NA&gt; NA ## # … with 1 more variable: lng &lt;dbl&gt; Right, now we’re going to use group_by() and tally() again, this time on the the wikititle, wikilat and wikilon columns. This is because the wikititle is a standardised title, which means it will group together cities properly, rather than giving a different row for slightly different combinations of the three geographic information columns (incidentally, it could also be used to link to wikidata) lc_with_geo_counts = lc_with_geo %&gt;% group_by(wikititle, lat, lng) %&gt;% tally(n) Now we’ve got a dataframe with counts of total newspapers, for each standardised wikipedia title in the dataset. knitr::kable(head(lc_with_geo_counts,10)) wikititle lat lng n Abbots_Langley 51.7010 -0.4160 1 Aberavon_(UK_Parliament_constituency) 51.6000 -3.8120 1 Aberdare 51.7130 -3.4450 20 Aberdeen 57.1500 -2.1100 82 Abergavenny 51.8240 -3.0167 9 Abergele 53.2800 -3.5800 8 Abersychan 51.7239 -3.0587 2 Abertillery 51.7300 -3.1300 2 Aberystwyth 52.4140 -4.0810 31 Abingdon-on-Thames 51.6670 -1.2830 23 OK, lc_with_geo_counts is what we want to plot. This contains the city title, coordinates and counts for all the relevant places in our dataset. But first we need the map we created earlier. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) + theme_void() Figure 6.4: Blank Map of UK and Ireland 6.4 Bringing it all together Now we will plot the cities using geom_point() We’ll specify the lc_with_geo_counts as the argument to data = within geom_point(). The x axis position of each point is the longitude, and the y axis the latitude. We’ll also use the argument size = n within the aes(), to tell ggplot2 to size the points by the column n, which contains the counts for each of our locations, and the argument alpha = .7 outside the aes(), to make the points more transparent and slightly easier to read overlapping ones. One last thing we’ll add is +scale_size_area(). This sizes the points using their radius rather than diameter, which is a more correct way of representing numbers using circles! Using labs(), add a title, and with scale_size_area() and scale_color_viridis_c(), make some changes to the size and colours, respectively. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) + theme_void() + geom_point(data = lc_with_geo_counts, aes(x = as.numeric(lng), y = as.numeric(lat), size = n, color = log(n)), alpha = .7) + scale_size_area(max_size = 8) + scale_color_viridis_c() + theme(legend.position = &#39;none&#39;) + theme(title = element_text(size = 12)) Figure 6.5: Finished Map 6.5 Drawing a newspaper titles ‘Choropleth’ map with R and the sf package Another type of map is known as a ‘choropleth’. This is where the data is visualised by a certain polygon area rather than a point. Typically these represent areas like parishes, counties or countries. Using the library sf, which stands for Simple Features, a choropleth map can be made quite quickly. A choropleth map uses a shapefile, which is a list of polygons and a projection. The trick here is to use the coordinates to correctly situate each set of points within the correct county, as found in the shapefile. Then, count up the titles by this corrected county, and use this total to color or shade the map. The good thing about this method is that once you have a set of coordinates, they can be situated within any shapefile - a historic map, for example. This is particularly useful for anything to do with English counties, which tend to be changed pretty regularly. This section uses data from .visionofbritain.co.uk, which needs to be downloaded separately. You could also use the free boundary data here: https://www.ordnancesurvey.co.uk/business-government/products/boundaryline, which contains bondaries for both modern and historic counties. This is an excellent source, and the file includes a range of boundaries including counties but also districts and constituencies, under an ‘Open Government Licence’. ## Choropleth map steps The steps to create this type of map: Download shapefiles for england and scotland from here Turn into sf object Download list of points, turn into sf object Use st join to get county information Join to the title list and deselect everything except county and titles - maybe 19th century only.. Join that to the sf object Plot using geom_sf() Load libraries library(tidyverse) library(sf) 6.6 Get county information from the title list Next, download (if you haven’t already) the title list from the British Library open repository. title_df = read_csv(&#39;data/BritishAndIrishNewspapersTitleList_20191118.csv&#39;) 6.7 Make the points object using geom_sf() 6.7.1 Download shapefiles First, download the relevant shapefiles. These don’t necessarily have to be historic ones. Use st_read() to read the file, specifying its path. Do this for England, Wales and Scotland (we don’t have points for Ireland). 6.8 Transform from UTM to lat/long using st_transform() These shapefiles use points system known as UTM, which stands for ‘Universal Transverse Mercator’. According to wikipedia, it differs from global latitude/longitude in that it divides earth into 60 zones and projects each to the plane as a basis for its coordinates. It needs to be transformed into lat/long coordinates, because the coordinates we have are in that format. This is easy with st_transform(). To transform correctly, the correct crs is needed. This is the code for which of the 60 zones this UTM comes from. Britain is 4326. eng_1851 = st_transform(eng_1851, crs = 4326) scot_1851 = st_transform(scot_1851, crs = 4326) Bind them both together, using rbind() to make one big shapefile for Great Britain. gb1851 = rbind(eng_1851, scot_1851 %&gt;% select(-UL_AUTH)) 6.9 Download and merge the title list with a set of coordinates. Next, load and pre-process the set of coordinates: geocorrected = read_csv(&#39;data/geocorrected.csv&#39;) Change the column names: library(snakecase) colnames(geocorrected) = to_snake_case(colnames(geocorrected)) Change some column names further, select just the relevant columns, change the NA values and get rid of any empty entries. colnames(geocorrected)[6:8] = c(&#39;wikititle&#39;, &#39;lat&#39;, &#39;lng&#39;) geocorrected = geocorrected %&gt;% select(-1, -9,-10, -11, -12) geocorrected = geocorrected %&gt;% mutate(country_of_publication = replace(country_of_publication, country_of_publication == &#39;na&#39;, NA)) %&gt;% mutate(general_area_of_coverage = replace(general_area_of_coverage, general_area_of_coverage == &#39;na&#39;, NA)) %&gt;% mutate(coverage_city = replace(coverage_city, coverage_city == &#39;na&#39;, NA)) geocorrected = geocorrected %&gt;% mutate(lat = as.numeric(lat)) %&gt;% mutate(lng = as.numeric(lng)) %&gt;% filter(!is.na(lat)) %&gt;% filter(!is.na(lng)) Next, join these points to the title list, so that every title now has a set of lat/long coordinates. title_df = title_df %&gt;% left_join(geocorrected) %&gt;% filter(!is.na(lat)) %&gt;% filter(!is.na(lng)) 6.10 Using st_join to connect the title list to the shapefile To join this to the shapefile, we need to turn it in to an simple features item. To do this we need to specify the coordinates and the CRS. The resulting file will contain a new column called ‘geometry’, containing the lat/long coordaintes in the correct simple features format. st_title = st_as_sf(title_df, coords = c(&#39;lng&#39;, &#39;lat&#39;)) st_title = st_title %&gt;% st_set_crs(4326) Now, we can use a special kind of join, which will join the points in the title list, if they are within a particular polygon. The resulting dataset now has the relevant county, as found in the shapefile. st_counties = st_join(st_title, gb1851) Make a new dataframe, containing just the counties and their counts. county_tally = st_counties %&gt;% select(G_NAME) %&gt;% group_by(G_NAME) %&gt;% tally() %&gt;% st_drop_geometry() 6.11 Draw using ggplot2 and geom_sf() Join this to the shapefile we made earlier, which gives a dataset with the relevant counts attached to each polygon. This can then be visualised using the geom_sf() function from ggplot2, and all of ggplot2’s other features can be used. gb1851 %&gt;% left_join(county_tally) %&gt;% ggplot() + geom_sf(lwd = .2,color = &#39;black&#39;, aes(fill = n)) + theme_void() + lims(fill = c(10,4000)) + scale_fill_viridis_c(option = &#39;plasma&#39;) + labs(title = &quot;British Library Newspaper\\nTitles, by County&quot;, fill = &#39;No. of Titles:&#39;) + theme(legend.position = &#39;left&#39;) + theme(title = element_text(size = 12), legend.title = element_text(size = 8)) Figure 6.6: Choropleth Made with geom_sf() Further reading: The book ‘Geocomputation with R’ is a fantastic resource for learning about mapping: https://geocompr.robinlovelace.net. It’s surprisinly easy to read even though it goes through some pretty advanced topics. "],
["download-and-unzip-newspaper-files-using-r.html", "7 Download and Unzip Newspaper Files Using R 7.1 Download a set of newspaper files from the British Library’s open repository 7.2 Folder structure 7.3 Contruct a Corpus 7.4 Bulk extract the files using unzip() and a for() loop", " 7 Download and Unzip Newspaper Files Using R 7.1 Download a set of newspaper files from the British Library’s open repository Newspaper data is available on the British Library’s open repository, and the first thing to do is to download and extract the data so that it can be worked on in the rest of the handbook. First, download some years you’re interested in from the Repository. The titles are available on the research repository here: https://bl.iro.bl.uk/collection/353c908d-b495-4413-b047-87236d2573e3, and more will be added with time. The repository is divided into collections and datasets. The British Library newspapers collection contains a series of datasets, each of which contains a number of files. Each of the files is a zip file containing all the METS/ALTO .xml for a single year of that title. For this tutorial, I’ve downloaded all the files for the Northern Daily Times and its linked titles, which are all available here. All these titles have a doi which makes them nicely suitable for reproducible code. From that link: “The Liverpool-based Northern Daily Times (1853-1961, with two changes of title) was the first provincial daily newspaper in England to enjoy a sustained run.” It will make a nice case study for some of the techniques in the tutorials. 7.2 Folder structure Each file will have a filename like this: BLNewspapers_TheSun_0002194_1850.zip To break it down in parts: BLNewspapers - this identifies the file as coming from the British Library TheSun - this is the title of the newspaper, as found on the Library’s catalogue. 0002194 - This is the NLP, a unique code given to each title. This code is also found on the Title-level list, in case you want to link the titles from the repository to that dataset. 1850 - The year. 7.3 Contruct a Corpus Construct a ‘corpus’ of newspapers, using whatever criteria you see fit. Perhaps you’re interested in a longitudinal study, and would like to download a small sample of years spread out over the century, or maybe you’d like to look at all the issues in a single newspaper, or perhaps all of a single year across a range of titles. Once you’ve downloaded your corpus, put the unextracted zip files in a folder called ‘newspapers’, within the working directory for this R project. If you’re using windows, you can use the method below to bulk extract the files. On mac, the files will unzip automatically, but they won’t merge: the OS will duplicate the folder with a sequential number after it - so you might need to move years from these extra folders into the first NLP folder. For example: if you download all the zip files from the link above, you’ll have some extra folders named like the below: You’ll need to move the year folders within these to the folders named 0002083, and so forth. 7.4 Bulk extract the files using unzip() and a for() loop R can be used to unzip the files in bulk, which is particularly useful if you have downloaded a large number of files. It’s very simple, there’s just two steps. This is useful if you’re using windows and have a large number of files to unzip. First, use list.files() to create a vector, called zipfile containing the full file paths to all the zip files in the ‘newspapers’ folder you’ve just created. zipfiles = list.files(&quot;newspapers/&quot;, full.names = TRUE) zipfiles Now, use this in a loop with unzip(). Loops in R are very useful for automating simple tasks. The below takes each file named in the ‘zipfiles’ vector, and unzips it. for(file in zipfiles){ unzip(file) } Once this is done, you’ll have a new (or several new) folders in the directory that you’re working in (not the newspapers directory). These are named using the NLP, so they should look like this in your project directory: To tidy up, put these back into the newspapers folder, so you have the following: Project folder-&gt; newspapers -&gt;group of NLP folders The next step is to extract the full text and put it into .csv files "],
["extract-text.html", "8 Make a Text Corpus 8.1 Where is this data? 8.2 Folder structure", " 8 Make a Text Corpus 8.1 Where is this data? Inspired by the posts by https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/ I’ve written some code to extract the full text of articles from our newspapers, and then do some basic text mining and topic modelling. I’ve taken the basic principle of the the tutorial above and tailored it heavily to the British Library’s METS/ALTO. In the British Library, the METS file contains information on textblocks. Each textblock has a code, which can be found in one of the ALTO files - of which there are one per page. The ALTO files list each individual word in each textblock. The METS file also contains information on which textblocks make up each article, which means that the newspaper can be recreated, article by article. The output will be a csv for each issue - these can be combined into a single dataframe afterwards, but it’s nice to have the .csv files themselves first of all. 8.2 Folder structure Download and extract the newspapers you’re interested in, and put them in the same folder as the project you’re working on in R. The folder structure of the newspapers is [nlp]-&gt;year-&gt;issue month and day-&gt; xml files. The nlp is a unique code given to each digitised newspaper. This makes it easy to find an individual issue of a newspaper. Load some libraries: all the text extraction is done using tidyverse and furrr for some parallel programming. require(furrr) ## Loading required package: furrr ## Loading required package: future require(tidyverse) library(tidytext) Basically there’s two main functions: get_page(), which extracts words and their corresponding textblock, and make_articles(), which extracts a table of the textblocks and corresponding articles, and joins them to the words from get_page(). Here’s get_page(): get_page = function(alto){ page = alto %&gt;% read_file() %&gt;% str_split(&quot;\\n&quot;, simplify = TRUE) %&gt;% keep(str_detect(., &quot;CONTENT|&lt;TextBlock ID=&quot;)) %&gt;% str_extract(&quot;(?&lt;=CONTENT=\\&quot;)(.*?)(?=WC)|(?&lt;=&lt;TextBlock ID=)(.*?)(?= HPOS=)&quot;)%&gt;% discard(is.na) %&gt;% as.tibble() %&gt;% mutate(pa = ifelse(str_detect(value, &quot;pa[0-9]{7}&quot;), str_extract(value, &quot;pa[0-9]{7}&quot;), NA)) %&gt;% fill(pa) %&gt;% filter(str_detect(pa, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;STYLE=\\&quot;subscript\\&quot; &quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;\\&quot;&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart2 SUBS_CONTENT=&quot;)) } I’ll break it down in stages: First read the alto page, which should be an argument to the function. Here’s one page to use as an example: alto = &quot;newspapers/0002083/1853/0924/0002083_18530924_0001.xml&quot; altofile = alto %&gt;% read_file() Split the file on each new line: altofile = altofile %&gt;% str_split(&quot;\\n&quot;, simplify = TRUE) altofile %&gt;% glimpse() ## chr [1, 1:3181] &quot;&lt;?xml version=\\&quot;1.0\\&quot; encoding=\\&quot;UTF-8\\&quot;?&gt;\\r&quot; ... Just keep lines which contain either a CONTENT or TextBlock tag: altofile = altofile %&gt;% keep(str_detect(., &quot;CONTENT|&lt;TextBlock ID=&quot;)) altofile %&gt;% glimpse() ## chr [1:1140] &quot;\\t\\t\\t\\t&lt;TextBlock ID=\\&quot;P1_TB00001\\&quot; HPOS=\\&quot;3800\\&quot; VPOS=\\&quot;9\\&quot; WIDTH=\\&quot;154\\&quot; HEIGHT=\\&quot;54\\&quot; STYLEREFS=\\&quot;TXT_0 PAR_LEFT\\&quot;&gt;\\r&quot; ... This was the bit I never would have figured out: it extracts the words and the textblock ID for each line. altofile = altofile %&gt;% str_extract(&quot;(?&lt;=CONTENT=\\&quot;)(.*?)(?=WC)|(?&lt;=&lt;TextBlock ID=)(.*?)(?= HPOS=)&quot;) %&gt;% discard(is.na) %&gt;% as_tibble() Now I have a dataframe with a single column, which is every textblock, textline and word in the ALTO file. Now we need to extract the textblock IDs, put them in a separate column, and then fill() each textblock ID down until it reaches the next one. altofile = altofile %&gt;% mutate(pa = ifelse(str_detect(value, &quot;pa[0-9]{7}&quot;), str_extract(value, &quot;pa[0-9]{7}&quot;), NA)) %&gt;% fill(pa) Now we get rid of the textblock lines in the column which should contain words, and get rid of some other tags which have come through: altofile = altofile %&gt;% filter(str_detect(pa, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;STYLE=\\&quot;subscript\\&quot; &quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;\\&quot;&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart2 SUBS_CONTENT=&quot;)) Ta-da! A dataframe with individual words on one side and the text block on the other. head(altofile) ## # A tibble: 6 x 2 ## value pa ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Silo&#39;s &quot; pa0001001 ## 2 &quot;MT, &quot; pa0001002 ## 3 &quot;Uor &quot; pa0001002 ## 4 &quot;Cabar &quot; pa0001003 ## 5 &quot;Chrini &quot; pa0001004 ## 6 &quot;Iron &quot; pa0001005 This is the second function: make_articles &lt;- function(foldername){ metsfilename = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*mets.xml&quot;) %&gt;% discard(is.na) csvfoldername = metsfilename %&gt;% str_remove(&quot;_mets.xml&quot;) metsfile = read_file(metsfilename) page_list = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*[0-9]{4}.xml&quot;) %&gt;% discard(is.na) metspagegroups = metsfile %&gt;% str_split(&quot;&lt;mets:smLinkGrp&gt;&quot;)%&gt;% flatten_chr() %&gt;% as_tibble() %&gt;% filter(str_detect(value, &#39;#art[0-9]{4}&#39;)) %&gt;% mutate(articleid = str_extract(value,&quot;[0-9]{4}&quot;)) future_map(page_list, get_page) %&gt;% bind_rows() %&gt;% left_join(extract_page_groups(metspagegroups$value) %&gt;% unnest() %&gt;% mutate(art = ifelse(str_detect(id, &quot;art&quot;), str_extract(id, &quot;[0-9]{4}&quot;), NA)) %&gt;% fill(art) %&gt;% filter(!str_detect(id, &quot;art[0-9]{4}&quot;)), by = c(&#39;pa&#39; = &#39;id&#39;)) %&gt;% group_by(art) %&gt;% summarise(text = paste0(value, collapse = &#39; &#39;)) %&gt;% mutate(issue_name = metsfilename ) %&gt;% write_csv(path = paste0(csvfoldername, &quot;.csv&quot;)) } It’s a bit more complicated, and a bit of a fudge. Because there are multiple ALTO pages for one METS file, we need to read in all the ALTO files, run our get_pages() function on them within this function, bind them altogether, and then join that to a METS file which contains an article ID and all the corresponding textBlocks. I’ll try to break it down into components: It takes an argument called ‘foldername’. This should be a list of issue folders - which is that final folder, in the format mmdd, which contains a single issue. we can pass a list of folder names using furrr, and it will run the function of each folder in turn. To break it down, with a single folder: foldername = &quot;newspapers/0002083/1853/0924/&quot; Using the folder name as the last part of the file path, and then a regular expression to get only a file ending in mets.xml, this will get the correct METS file name and read it into memory: metsfilename = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*mets.xml&quot;) %&gt;% discard(is.na) metsfile = read_file(metsfilename) We also need to call the .csv (which we’re going to have as an output) a unique name: csvfoldername = metsfilename %&gt;% str_remove(&quot;_mets.xml&quot;) Next we have to grab all the ALTO files in the same folder, using the same method: page_list = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*[0-9]{4}.xml&quot;) %&gt;% discard(is.na) Next we need the file which lists all the pagegroups and corresponding articles. metspagegroups = metsfile %&gt;% str_split(&quot;&lt;mets:smLinkGrp&gt;&quot;) %&gt;% flatten_chr() %&gt;% as_tibble() %&gt;% filter(str_detect(value, &#39;#art[0-9]{4}&#39;)) %&gt;% mutate(articleid = str_extract(value,&quot;[0-9]{4}&quot;)) The next bit uses a function written by brodrigues called extractor() extractor &lt;- function(string, regex, all = FALSE){ if(all) { string %&gt;% str_extract_all(regex) %&gt;% flatten_chr() %&gt;% str_extract_all(&quot;[:alnum:]+&quot;, simplify = FALSE) %&gt;% map(paste, collapse = &quot;_&quot;) %&gt;% flatten_chr() } else { string %&gt;% str_extract(regex) %&gt;% str_extract_all(&quot;[:alnum:]+&quot;, simplify = TRUE) %&gt;% paste(collapse = &quot; &quot;) %&gt;% tolower() } } We also need another function which extracts the correct pagegroups: extract_page_groups &lt;- function(article){ id &lt;- article %&gt;% extractor(&quot;(?&lt;=&lt;mets:smLocatorLink xlink:href=\\&quot;#)(.*?)(?=\\&quot; xlink:label=\\&quot;)&quot;, all = TRUE) type = tibble::tribble(~id, id) } Next this takes the list of ALTO files, and applies the get_page() function to each item, then binds the four files together vertically. I’ll give it a random variable name, even though it doesn’t need one in the function because we just pipe it along to the csv. t = future_map(page_list, get_page) %&gt;% bind_rows() head(t) This extracts the page groups from the mets dataframe we made, and turns it into a dataframe with the article ID as a number, again extracting and filtering using regular expressions, and using fill(). The result is a dataframe of every word, plus their article and text block. t = t %&gt;% left_join(extract_page_groups(metspagegroups$value) %&gt;% unnest() %&gt;% mutate(art = ifelse(str_detect(id, &quot;art&quot;), str_extract(id, &quot;[0-9]{4}&quot;), NA))%&gt;% fill(art), by = c(&#39;pa&#39; = &#39;id&#39;)) %&gt;% fill(art) head(t, 50) Next we use summarise() and paste() to group the words into the individual articles, and add the mets filename so that we also can extract the issue date afterwards. t = t %&gt;% group_by(art) %&gt;% summarise(text = paste0(value, collapse = &#39; &#39;)) %&gt;% mutate(issue_name = metsfilename ) head(t, 10) And finally write to .csv using the csvfoldername we created: t %&gt;% write_csv(path = paste0(csvfoldername, &quot;.csv&quot;)) To run it on a bunch of folders, you’ll need to make a list of paths to all the issue folders you want to process. You can do this using list_dirs. You only want these final-level issue folders, otherwise it will try to work on an empty folder and give an error. This means that if you want to work on multiple years or issues, you’ll need to figure out how to pass a list of just the isuse level folder paths. Here I’ve done it by excluding any pathnames which are shorter than the correct number for an issue-level folder. folderlist = list.dirs(&quot;newspapers/0002083/&quot;, full.names = TRUE, recursive = TRUE) folderlist = folderlist[nchar(folderlist)&gt;24] Finally, this applies the function make_articles() to everything in the folderslist vector. It will write a new .csv file into each of the folders, containing the article text and codes. You can add whatever folders you like to a vector called folderlist, and it will generate a csv of articles for each one. future_map(folderlist, make_articles) Do the same for the other two NLPs: folderlist = list.dirs(&quot;newspapers/0002084/&quot;, full.names = TRUE, recursive = TRUE) folderlist = folderlist[nchar(folderlist)&gt;24] Finally, this applies the function make_articles() to everything in the folderslist vector. It will write a new .csv file into each of the folders, containing the article text and codes. You can add whatever folders you like to a vector called folderlist, and it will generate a csv of articles for each one. future_map(folderlist, make_articles) folderlist = list.dirs(&quot;newspapers/0002085/&quot;, full.names = TRUE, recursive = TRUE) folderlist = folderlist[nchar(folderlist)&gt;24] Finally, this applies the function make_articles() to everything in the folderslist vector. It will write a new .csv file into each of the folders, containing the article text and codes. You can add whatever folders you like to a vector called folderlist, and it will generate a csv of articles for each one. future_map(folderlist, make_articles) It’s not very fast (I think it can take 10 or 20 seconds per issue, so bear that in mind), but eventually you should now have a .csv file in each of the issue folders, with a row per line. Some last pre-processing steps: Make a dataframe containing all the .csv files, which is easier to work with, add a unique code for each article, and extract date and title information from the file path (I’ve put more information on this in comments in the code below). Save this to R’s data format using save(), for use later. news_sample_dataframe = list.files(&quot;newspapers/&quot;, pattern = &quot;csv&quot;, recursive = TRUE, full.names = TRUE) %&gt;% map_df(~read_csv(.)) %&gt;% #filter(is.na(headline)) %&gt;% # These two lines take all the newly created csv files and read them into one dataframe separate(issue_name, into = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;), sep = &quot;/&quot;) %&gt;% # separate the filename into some arbitrarily named colums select(art, text, b, d, e) %&gt;% # select the ones which contain information on the title and date select(art, text, title = b, year = d, date = e) %&gt;% # change their names mutate(full_date = as.Date(paste0(year,date), &#39;%Y%m%d&#39;)) %&gt;% # make a date column, and turn it into date format mutate(article_code = 1:n()) %&gt;% # give every article a unique code select(article_code, everything())# select all columns save(news_sample_dataframe, file = &#39;news_sample_dataframe&#39;) This news_sample_dataframe will be used for some basic text mining examples: word frequency count tf-idf scores sentiment analysis topic modelling text reuse "],
["term-frequencies.html", "9 Term Frequencies 9.1 Load the news dataframe and relevant libraries 9.2 Tokenise the text using unnest_tokens() 9.3 Pre-process to clean and remove stop words 9.4 Create and save a dataset of tokenised text 9.5 Count the tokens 9.6 Visualise the Results 9.7 Further reading", " 9 Term Frequencies The first thing you might want to do with a large dataset of text is to count the words within it. Doing this with newspaper data can be particularly significant, because it’s quite easy to discover trends, reporting practices, and particular events. By count words, and sorting by date, or by title, it’s possible to make some interesting comparisons and conclusions in the makeup of different titles, or to understand changes in reporting over time. Again, as this is a small sample, the conclusions will be light, but the aim is the show the method. As an example, here is a Shiny application using a sample of text from British Library newspapers. Search for a word below and browse through a number of ways to visualise it. 9.1 Load the news dataframe and relevant libraries The first thing is to take the newss dataframe, as made in the previous chapter, and load it into memory, if it isn’t already. load(&#39;news_sample_dataframe&#39;) The two libraries we’ll use are tidyverse, as usual, and tidytext. The dataframe we created has a row per article. This is a really easy format to do text mining with, using the techniques from here: https://www.tidytextmining.com/, and the library tidytext. If it’s not installed, use ```install.packages(‘tidytext’) to install it. library(tidyverse) library(tidytext) Take a quick look at the dataframe: glimpse(news_sample_dataframe) ## Rows: 178,725 ## Columns: 7 ## $ article_code &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ art &lt;chr&gt; &quot;0001&quot;, &quot;0002&quot;, &quot;0003&quot;, &quot;0004&quot;, &quot;0005&quot;, &quot;0006&quot;, &quot;0007&quot;, … ## $ text &lt;chr&gt; &quot;Silo&#39;s&quot;, &quot;MT, Uor&quot;, &quot;Cabar&quot;, &quot;Chrini&quot;, &quot;Iron Foundry,… ## $ title &lt;chr&gt; &quot;0002083&quot;, &quot;0002083&quot;, &quot;0002083&quot;, &quot;0002083&quot;, &quot;0002083&quot;, &quot;… ## $ year &lt;chr&gt; &quot;1853&quot;, &quot;1853&quot;, &quot;1853&quot;, &quot;1853&quot;, &quot;1853&quot;, &quot;1853&quot;, &quot;1853&quot;, … ## $ date &lt;chr&gt; &quot;0924&quot;, &quot;0924&quot;, &quot;0924&quot;, &quot;0924&quot;, &quot;0924&quot;, &quot;0924&quot;, &quot;0924&quot;, … ## $ full_date &lt;date&gt; 1853-09-24, 1853-09-24, 1853-09-24, 1853-09-24, 1853-09… 9.2 Tokenise the text using unnest_tokens() Most analysis involves tokenising the text. This divides the text into ‘tokens’ - representing one unit. A unit is often a word, but could be a bigram - a sequence of two consecutive words, or a trigram, a sequence of three consecutive words. With the library tidytext, this is done using a function called unnest_tokens(). This will split the column containing the text of the article into a long dataframe, with one word per row. The two most important arguments to ``unnest_tokensareoutputandinput```. This is fairly self explanatory. Just pass it the name you would like to give the new column of words (or n-grams) and the column you’d like to split up: in this case the original column is called ‘text’, and we’d like our column of words to be called words. news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text) %&gt;% head(10) ## # A tibble: 10 x 7 ## article_code art title year date full_date word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 0001 0002083 1853 0924 1853-09-24 silo&#39;s ## 2 2 0002 0002083 1853 0924 1853-09-24 mt ## 3 2 0002 0002083 1853 0924 1853-09-24 uor ## 4 3 0003 0002083 1853 0924 1853-09-24 cabar ## 5 4 0004 0002083 1853 0924 1853-09-24 chrini ## 6 5 0005 0002083 1853 0924 1853-09-24 iron ## 7 5 0005 0002083 1853 0924 1853-09-24 foundry ## 8 5 0005 0002083 1853 0924 1853-09-24 ei ## 9 6 0006 0002083 1853 0924 1853-09-24 barrister ## 10 6 0006 0002083 1853 0924 1853-09-24 ac You can also specify an argument for token, allowing you to split the text into sentences, characters, lines, or n-grams.If you split into n-grams, you need to use the argument n= to specify how many consecutive words you’d like to use. Like this: news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text, token = &#39;ngrams&#39;, n =3) ## # A tibble: 81,795,495 x 7 ## article_code art title year date full_date word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 0001 0002083 1853 0924 1853-09-24 &lt;NA&gt; ## 2 2 0002 0002083 1853 0924 1853-09-24 &lt;NA&gt; ## 3 3 0003 0002083 1853 0924 1853-09-24 &lt;NA&gt; ## 4 4 0004 0002083 1853 0924 1853-09-24 &lt;NA&gt; ## 5 5 0005 0002083 1853 0924 1853-09-24 iron foundry ei ## 6 6 0006 0002083 1853 0924 1853-09-24 barrister ac i ## 7 6 0006 0002083 1853 0924 1853-09-24 ac i dated ## 8 6 0006 0002083 1853 0924 1853-09-24 i dated th ## 9 7 0007 0002083 1853 0924 1853-09-24 things re r ## 10 7 0007 0002083 1853 0924 1853-09-24 re r quired ## # … with 81,795,485 more rows 9.3 Pre-process to clean and remove stop words Before we do any counting, there’s a couple more processing steps. I’m going to remove ‘stop words’. Stop words are very frequently-used words which often crowd out more interesting results. This isn’t always the case, and you shoudln’t just automatically get rid of them, but rather think about what it is yo uare looking for. For this tutorial, though, the results will be more interesting if it’s not just a bunch of ‘the’ and ‘at’ and so forth. This is really easy. We load a dataframe of stopwords, which is included in the tidytext package. data(&quot;stop_words&quot;) Next use the function anti_join(). This bascially removes any word in our word list which is also in the stop words list news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; ## # A tibble: 44,126,361 x 7 ## article_code art title year date full_date word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 0001 0002083 1853 0924 1853-09-24 silo&#39;s ## 2 2 0002 0002083 1853 0924 1853-09-24 mt ## 3 2 0002 0002083 1853 0924 1853-09-24 uor ## 4 3 0003 0002083 1853 0924 1853-09-24 cabar ## 5 4 0004 0002083 1853 0924 1853-09-24 chrini ## 6 5 0005 0002083 1853 0924 1853-09-24 iron ## 7 5 0005 0002083 1853 0924 1853-09-24 foundry ## 8 5 0005 0002083 1853 0924 1853-09-24 ei ## 9 6 0006 0002083 1853 0924 1853-09-24 barrister ## 10 6 0006 0002083 1853 0924 1853-09-24 ac ## # … with 44,126,351 more rows A couple of words from the .xml have managed to sneak through our text processing: ‘style’ and ‘superscript’. I’m also going to remove these, plus a few more common OCR errors for the word ‘the’. I’m also going to remove any word with two or less characters, and any numbers. Again, these are optional steps. I’ll store the dataframe as a variable called ‘tokenised_news_sample’. I’ll also save it using save(), which turns it into an .rdata file, which can be used later. 9.4 Create and save a dataset of tokenised text tokenised_news_sample = news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) %&gt;% filter(!word %in% c(&#39;superscript&#39;, &#39;style&#39;, &#39;de&#39;, &#39;thle&#39;, &#39;tile&#39;, &#39;tie&#39;, &#39;tire&#39;, &#39;tiie&#39;, &#39;tue&#39;, &#39;amp&#39;)) %&gt;% filter(!str_detect(word, &#39;[0-9]{1,}&#39;)) %&gt;% filter(nchar(word) &gt; 2) ## Joining, by = &quot;word&quot; save(tokenised_news_sample, file = &#39;tokenised_news_sample&#39;) 9.5 Count the tokens Now I can use all the tidyverse commands like filter, count, tally and so forth on the data, making it really easy to do basic analysis like word frequency counting. It’s a large list of words (about 35 million), so these processes might take a few seconds, even on a fast computer. A couple of examples: 9.5.1 The top words overall: tokenised_news_sample %&gt;% group_by(word) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% head(20) ## # A tibble: 20 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 street 388165 ## 2 liverpool 275023 ## 3 london 105531 ## 4 apply 104380 ## 5 john 93112 ## 6 day 92387 ## 7 office 80927 ## 8 house 77802 ## 9 o&#39;clock 76478 ## 10 time 70735 ## 11 tons 66083 ## 12 ship 62583 ## 13 york 60786 ## 14 public 57786 ## 15 water 57419 ## 16 steam 56530 ## 17 saturday 51932 ## 18 messrs 51533 ## 19 north 51247 ## 20 stock 51031 9.5.2 The top five words for each day in the dataset: tokenised_news_sample %&gt;% group_by(full_date, word) %&gt;% tally() %&gt;% arrange(full_date, desc(n)) %&gt;% group_by(full_date) %&gt;% top_n(5) %&gt;% head(100) ## Selecting by n ## # A tibble: 100 x 3 ## # Groups: full_date [18] ## full_date word n ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; ## 1 1853-09-24 liverpool 9 ## 2 1853-09-24 lat 5 ## 3 1853-09-24 cent 3 ## 4 1853-09-24 gemouth 3 ## 5 1853-09-24 hat 3 ## 6 1853-09-24 street 3 ## 7 1853-09-24 time 3 ## 8 1853-09-24 york 3 ## 9 1853-09-26 day 15 ## 10 1853-09-26 cents 14 ## # … with 90 more rows 9.5.3 Check the top words per title (well, variant titles in this case): tokenised_news_sample %&gt;% group_by(title, word) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% group_by(title) %&gt;% top_n(5) ## Selecting by n ## # A tibble: 15 x 3 ## # Groups: title [3] ## title word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 0002084 street 185690 ## 2 0002084 liverpool 119109 ## 3 0002083 street 113206 ## 4 0002083 liverpool 99662 ## 5 0002085 street 89269 ## 6 0002085 liverpool 56252 ## 7 0002084 apply 53790 ## 8 0002084 london 43705 ## 9 0002084 office 40075 ## 10 0002083 john 35981 ## 11 0002083 london 33627 ## 12 0002083 apply 32387 ## 13 0002085 london 28199 ## 14 0002085 day 21016 ## 15 0002085 apply 18203 You can also summarise by units of time, using the function cut(). This rounds the date down to the nearest day, year or month. Once it’s been rounded down, we can count by this new value. 9.5.4 Top words by year tokenised_news_sample %&gt;% mutate(year = cut(full_date, &#39;year&#39;)) %&gt;% group_by(year, word) %&gt;% tally() %&gt;% arrange(year, desc(n)) %&gt;% group_by(year) %&gt;% top_n(5) ## Selecting by n ## # A tibble: 30 x 3 ## # Groups: year [6] ## year word n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 1853-01-01 liverpool 9231 ## 2 1853-01-01 street 8571 ## 3 1853-01-01 nov 6147 ## 4 1853-01-01 oct 5527 ## 5 1853-01-01 sept 4337 ## 6 1856-01-01 street 59297 ## 7 1856-01-01 liverpool 52905 ## 8 1856-01-01 john 19469 ## 9 1856-01-01 london 18638 ## 10 1856-01-01 tons 17566 ## # … with 20 more rows 9.6 Visualise the Results We can also pipe everything directly to a plot. ‘Ship’ is a common word: did its use change over time? Here we use filter() to filter out everything except the word (or words) we’re interested in. For this to be in any way meaningful, you should think of some way of normalising the results, so that the number is of a percentage of the total words in that title, for example. The raw numbers may just indicate a change in the total volume of text. 9.6.1 Words over time tokenised_news_sample %&gt;% filter(word == &#39;ship&#39;) %&gt;% group_by(full_date, word) %&gt;% tally() %&gt;% ggplot() + geom_col(aes(x = full_date, y = n)) Figure 9.1: Chart of the Word ‘ship’ over time 9.6.2 Chart several words over time Charting a couple of words might be more interesting: How about ‘steam’ versus ‘sail’? tokenised_news_sample %&gt;% filter(word %in% c(&#39;steam&#39;, &#39;sail&#39;)) %&gt;% group_by(full_date, word) %&gt;% tally() %&gt;% ggplot() + geom_col(aes(x = full_date, y = n, fill = word)) Figure 9.2: Charting Several Words Over the Entire Dataset 9.7 Further reading As usual, the best place to learn more is by reading the ‘Tidy Text Mining’ book available at https://www.tidytextmining.com. "],
["calculating-tf-idf-scores-with-tidytext.html", "10 Calculating tf-idf Scores with Tidytext", " 10 Calculating tf-idf Scores with Tidytext Another common analysis of text uses a metric known as ‘tf-idf’. This stands for term frequency-inverse document frequency. Take a corpus with a bunch of documents (here we’re using articles as individual documents). TF-idf scores the words in each document, normalised by how often they are found in the other documents. It’s a good measure of the ‘importance’ of a particular word for a given document, and it’s particularly useful in getting good search results from keywords. It’s also a way of understanding the way language is used in newspapers, and how it changed over time. The function in the tidytext library bind_tf_idf takes care of all this. First you need to get a frequency count for each issue in the dataframe. We’ll make a unique issue code by pasting together the date and the nlp into one string, using the function paste0, and save this as a file named ‘issue_words’. First load the necessary libraries and tokenised data we created in the last notebook: library(tidytext) library(tidyverse) library(rmarkdown) ## ## Attaching package: &#39;rmarkdown&#39; ## The following object is masked from &#39;package:future&#39;: ## ## run load(&#39;tokenised_news_sample&#39;) issue_words = tokenised_news_sample %&gt;% mutate(issue_code = paste0(title, full_date)) %&gt;% group_by(issue_code, word) %&gt;% tally() %&gt;% arrange(desc(n)) Next use bind_tf_idf() to calculate the measurement for each word in the dataframe. issue_words %&gt;% bind_tf_idf(word, issue_code, n) ## # A tibble: 14,071,360 x 6 ## # Groups: issue_code [1,358] ## issue_code word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00020851860-02-29 street 631 0.0220 0 0 ## 2 00020841859-02-18 street 583 0.0176 0 0 ## 3 00020841859-01-07 street 570 0.0179 0 0 ## 4 00020841859-01-01 street 564 0.0174 0 0 ## 5 00020841859-04-02 street 553 0.0169 0 0 ## 6 00020841859-02-24 street 552 0.0169 0 0 ## 7 00020841859-03-05 street 548 0.0183 0 0 ## 8 00020841859-03-12 street 537 0.0168 0 0 ## 9 00020841859-04-04 street 537 0.0164 0 0 ## 10 00020841859-03-10 street 532 0.0162 0 0 ## # … with 14,071,350 more rows Now we can sort it in descending order of the issue code, to find the most ‘unusual’ words: issue_words %&gt;% bind_tf_idf(word, issue_code, n) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 14,071,360 x 6 ## # Groups: issue_code [1,358] ## issue_code word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00020831853-09-24 gemouth 3 0.00441 7.21 0.0318 ## 2 00020831853-09-24 branchns 2 0.00294 7.21 0.0212 ## 3 00020831856-05-19 tetanus 86 0.00494 4.04 0.0199 ## 4 00020831853-09-24 iitio 2 0.00294 6.12 0.0180 ## 5 00020831856-06-18 dolley 27 0.00176 7.21 0.0127 ## 6 00020831853-12-29 decimal 88 0.00414 3.04 0.0126 ## 7 00020831856-07-19 strychnia 58 0.00331 3.69 0.0122 ## 8 00020831856-08-08 exhibitor 55 0.00310 3.75 0.0116 ## 9 00020851860-08-22 antimony 81 0.00396 2.69 0.0107 ## 10 00020831853-09-24 a:eof 1 0.00147 7.21 0.0106 ## # … with 14,071,350 more rows What does this tell us? Well, unfortunately, most of the ‘unusual’ words by this measure are OCR errors or spelling mistakes. One way to correct for this is to only include words in an English language dictionary. Use the lexicon package and then the command data(grady_augmented) to download a dictionary of English language words and common proper nouns, as a character vector: library(lexicon) data(grady_augmented) Get tf-idf scores again, filtering the dataset first to include only words within grady_augmented issue_words %&gt;% filter(word %in% grady_augmented) %&gt;% bind_tf_idf(word, issue_code, n) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 8,244,399 x 6 ## # Groups: issue_code [1,358] ## issue_code word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00020831856-05-19 tetanus 86 0.00612 4.04 0.0247 ## 2 00020831853-09-24 genna 1 0.00292 5.83 0.0170 ## 3 00020831853-12-29 decimal 88 0.00537 3.04 0.0163 ## 4 00020831856-06-18 dolley 27 0.00223 7.21 0.0161 ## 5 00020831856-08-08 exhibitor 55 0.00415 3.75 0.0156 ## 6 00020831853-09-24 fearer 1 0.00292 5.27 0.0154 ## 7 00020831853-09-24 fredrick 1 0.00292 5.27 0.0154 ## 8 00020831853-09-24 shawm 1 0.00292 5.27 0.0154 ## 9 00020831853-09-24 shifters 1 0.00292 5.27 0.0154 ## 10 00020831853-09-24 vising 1 0.00292 5.13 0.0150 ## # … with 8,244,389 more rows The highest tf-idf score is for the word ‘tetanus’ on 19th May, 1856. This means that this word occurred lots of times in this issue, and not very often in other issues. This might point to particular topics, and it might, in particular, point to topics which had a very short or specific lifespan. If we had a bigger dataset, or one arranged in another way, these words might point to linguistic differences between regions, publishers, or writers. Let’s find the tetanus articles. We can use a function called str_detect() with filter() to filter to just articles containing a given word. So we’ll go back to the untokenised dataframe. load(&#39;news_sample_dataframe&#39;) news_sample_dataframe %&gt;% filter(str_detect(text, &quot;tetanus&quot;)) ## # A tibble: 33 x 7 ## article_code art text title year date full_date ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 12355 0044 &quot;what quantity of strychn… 0002… 1856 0311 1856-03-11 ## 2 16391 0016 &quot;heffield, and LinColn, 2… 0002… 1856 0515 1856-05-15 ## 3 16497 &lt;NA&gt; &quot;THE RIJGELEY POISONINGS. … 0002… 1856 0516 1856-05-16 ## 4 16534 0037 &quot;POLICE COURT.-YESTERDAY (… 0002… 1856 0517 1856-05-17 ## 5 16537 0040 &quot;am glad you have come … 0002… 1856 0517 1856-05-17 ## 6 16607 0030 &quot;CENTRAL AMERICA It is v… 0002… 1856 0519 1856-05-19 ## 7 16609 0032 &quot;14e Attorney -General : … 0002… 1856 0519 1856-05-19 ## 8 16611 0034 &quot;son Cie from the admini… 0002… 1856 0519 1856-05-19 ## 9 16668 0036 &quot;THEATRE ROYAL an English… 0002… 1856 0520 1856-05-20 ## 10 16726 0029 &quot;THE NORTHERN TIMES.---LIV… 0002… 1856 0521 1856-05-21 ## # … with 23 more rows These disproportionately high mentions of the word tetanus seem to be related to the trial of William Palmer (https://en.wikipedia.org/wiki/William_Palmer_(murderer), who was convicted for the murder of his friend by strychnine - which apparently caused tetanus. "],
["sentiment-analysis.html", "11 Sentiment analysis 11.1 Install and load relevant packages 11.2 Fetch sentiment data 11.3 Load the tokenised news sample 11.4 Charting Changes in Sentiment Over Time", " 11 Sentiment analysis A surprisingly easy text mining task, once your documents have been turned into a tokenised dataframe, is sentiment analysis. Sentiment analysis is the name for a range of techniques which attempt to measure emotion in a text. There are lots of ways of doing this, which become more and more sophisticated. One fairly simple but robust method is to take a dataset of words with corresponding sentiment scores (this could be a simple negative or positive score, or a score for each of a range of emotions). Then you join these scores to your tokenised dataframe, and count them. The tricky bit is working out what it all means: You could argue that it’s reductive to reduce a text to the sum of its positive and negative scores for each word - this is obviously not the way that language works. Also, if you’re summing the scores, you need to think about the unit you’re summarising by. Can you measure the emotions of a newspaper? or does it have to be per article? And of course it goes without saying that this was created by modern readers for use on modern text. Despite these questions, it can throw up some interesting patterns. Perhaps, if used correctly, one might be able to understand something of the way an event was reported, though it may not actually help with the ‘sentiment’ of the article, but rather reporting style or focus. I think with the right use, sentiment shows some promise when specifically applied to newspaper data, but thinking of it as sentiment may be a fool’s errand: it tells us something about the focus or style of an article, and over time and in bulk, something of a newspaper’s style or change in style. The tidytext library has a few built-in sentiment score datasets (or lexicons). To load them first install the textdata and tidytext packages, if they’re not installed already (using install.packages()) 11.1 Install and load relevant packages library(textdata) library(tidytext) library(tidyverse) 11.2 Fetch sentiment data Next use a function in the tidytext library called get_sentiments(). All this does is retrieve a dataset of sentiment scores and store them as a dataframe. There are four to choose from - I’ll quickly explain each one. 11.2.1 Afinn dataset afinnsentiments = get_sentiments(&#39;afinn&#39;) head(afinnsentiments,10) ## # A tibble: 10 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 The Afinn dataset has two colums: words in one column, and a value between -5 and +5 in the other. The value is a numeric score of the word’s perceived positivity or negativity. More information is available on the official project GitHub page 11.2.2 Bing dataset The second, the Bing dataset, was compiled by the researchers Minqing Hu and Bing Liu. It is also a list of words, with each classified as either positive or negative. bingsentiments = get_sentiments(&#39;bing&#39;) head(bingsentiments,10) ## # A tibble: 10 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative 11.2.3 Loughran dataset I’ve never used it, but it’s clearly similar to the Bing dataset, with a column of words and a classification of either negative or positive. More information and the original files can be found on the creator’s website loughransentiments = get_sentiments(&#39;loughran&#39;) head(loughransentiments,10) ## # A tibble: 10 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abandon negative ## 2 abandoned negative ## 3 abandoning negative ## 4 abandonment negative ## 5 abandonments negative ## 6 abandons negative ## 7 abdicated negative ## 8 abdicates negative ## 9 abdicating negative ## 10 abdication negative 11.2.4 NRC dataset nrcsentiments = get_sentiments(&#39;nrc&#39;) head(nrcsentiments,10) ## # A tibble: 10 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing. (https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) The NRC dataset is a bit different to the other ones. This time, there’s a list of words, and an emotion associated with that word. A word can have multiple entries, with different emotions attached to them. 11.3 Load the tokenised news sample load(&#39;tokenised_news_sample&#39;) This has two colums, ‘word’ and ‘value’. inner_join() will allow you to merge this with the tokenised dataframe. tokenised_news_sample %&gt;% inner_join(afinnsentiments) ## Joining, by = &quot;word&quot; ## # A tibble: 2,056,429 x 8 ## article_code art title year date full_date word value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 11 0011 0002083 1853 0924 1853-09-24 favor 2 ## 2 12 0012 0002083 1853 0924 1853-09-24 blind -1 ## 3 16 0016 0002083 1853 0924 1853-09-24 worth 2 ## 4 16 0016 0002083 1853 0924 1853-09-24 ban -2 ## 5 16 0016 0002083 1853 0924 1853-09-24 benefit 2 ## 6 28 0028 0002083 1853 0924 1853-09-24 united 1 ## 7 31 0031 0002083 1853 0924 1853-09-24 bankrupt -3 ## 8 36 0036 0002083 1853 0924 1853-09-24 mad -3 ## 9 40 0040 0002083 1853 0924 1853-09-24 resolve 2 ## 10 40 0040 0002083 1853 0924 1853-09-24 ass -4 ## # … with 2,056,419 more rows Now we have a list of all the words, one per line, which occurred in the afinn list, and their individual score. To make this in any way useful, we need to summarise the scores. The article seems by far the most logical start. We can get the average score for each article, which will tell us whether the article contained more positive or negative words. For this we use tally() and mean() I’m also using add_tally() to filter out only articles which contain at least 20 of these words from the lexicon, because I think it will make the score more meaningful. Let’s look at the most ‘positive’ article tokenised_news_sample %&gt;% inner_join(afinnsentiments) %&gt;% group_by(article_code) %&gt;% add_tally() %&gt;% filter(n&gt;20) %&gt;% tally(mean(value)) %&gt;% arrange(desc(n)) ## Joining, by = &quot;word&quot; ## # A tibble: 26,356 x 2 ## article_code n ## &lt;int&gt; &lt;dbl&gt; ## 1 31489 2.7 ## 2 33257 2.64 ## 3 53197 2.61 ## 4 17309 2.56 ## 5 53655 2.52 ## 6 54015 2.52 ## 7 32873 2.52 ## 8 7114 2.5 ## 9 52521 2.48 ## 10 6675 2.48 ## # … with 26,346 more rows 11.4 Charting Changes in Sentiment Over Time Sentiment analysis should be uesd with caution, but it’s potentially a useful tool, particularly to look at changes over time, or differences between newspapers or authors. We can plot the average of all the average article scores. If we had them, this could be segmented by title. Here I’ve charted all the sentiments found in the sample dataframe - but note that because the sample is so sparse and uneven, I’ve not spaced the dates temporally, but rather just one after another. tokenised_news_sample %&gt;% inner_join(afinnsentiments) %&gt;% group_by(full_date,article_code) %&gt;% add_tally() %&gt;% filter(n&gt;20) %&gt;% tally(mean(value)) %&gt;% group_by(full_date) %&gt;% tally(mean(n)) %&gt;% arrange(desc(n)) %&gt;% ggplot() + geom_col(aes(x = as.character(full_date), y = n)) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) Figure 11.1: Sentiment Over Time "],
["topic-modelling.html", "12 Topic modelling 12.1 Topic modelling with the library ‘topicmodels’ 12.2 Load the tokenised dataframe 12.3 Create a dataframe of word counts with tf_idf scores 12.4 Make a ‘document term matrix’", " 12 Topic modelling Another text mining tool which can be useful in exploring newspaper data is topic modelling. Topic modelling tried to discern a bunch of ‘topics’, expressed as a set of important keywords, from a group of documents. It works best if each document covers one clear subject. To demonstrate the value to newspaper data, I’ll talk through an example of applying the tutorial as written here: https://www.tidytextmining.com 12.1 Topic modelling with the library ‘topicmodels’ First, load and install the relevant lirbraries. If you’ve loaded the notebooks into Rstudio, it should detect and ask if you want to install any missing ones, but you might need to use install_packages() if not. library(tidyverse) library(tidytext) library(topicmodels) 12.2 Load the tokenised dataframe Load the tokenised dataframe created in the term_frequency notebook - this is the dataframe with one word per row. load(&#39;tokenised_news_sample&#39;) 12.3 Create a dataframe of word counts with tf_idf scores Like in the tf_idf notebook, make a dataframe of the words in each document, with the count and the tf-idf score. This will be used to filter and weight the text data. First, get the word counts for each article. An optional step would be to filter to include only English-language words and some common nouns. issue_words = tokenised_news_sample %&gt;% filter(word %in% grady_augmented) %&gt;% mutate(issue_code = paste0(title, full_date)) %&gt;% group_by(issue_code, word) %&gt;% tally() %&gt;% arrange(desc(n)) Next, use bind_tf_idf() to get the tf_idf scores. issue_words = issue_words %&gt;% bind_tf_idf(word, issue_code, n) 12.4 Make a ‘document term matrix’ Using the function cast_dtm() from the topicmodels package, make a document term matrix. This is a matrix with all the documents on one axis, all the words on the other, and the number of times that word appears as the value. We’ll also filter out words with a low tf-idf score, and only include words that occur at least 5 times. dtm_long &lt;- issue_words %&gt;% filter(tf_idf &gt; 0.00006) %&gt;% filter(n&gt;5) %&gt;% cast_dtm(issue_code, word, n) Use the LDA() function to produce the model. You specify the number of topics in advance, using the argument k, and we’ll set the random seed to a set number for reproducibility. The method algorithm used here is known as Latent Dirichlet Allocation - it uses the distribution of words in each document to group them together into ‘topics’. It can take some time to run the model - be prepared to wait a bit. lda_model_long_1 &lt;- LDA(dtm_long,k = 25, control = list(seed = 1234)) There are two things we can do now: first, get a list of words scored by how they contributed to each of the topics. result &lt;- tidytext::tidy(lda_model_long_1, &#39;beta&#39;) ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. We can plot the top words which make up each of the topics, to get an idea of how the articles have been categorised. Some of these make sense: there’s a topic which seems to be about university and education, one with words relating to poor laws, and a couple about disease in the army, as well as some more which contain words probably related to the Crimean war. result %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol = 4) + coord_flip() Figure 12.1: Sample Topics You can also group the articles by their percentage of each ‘topic’, and use this to find common thread between them - for more on this, see here: https://www.tidytextmining.com/topicmodeling.html "],
["detecting-text-reuse-in-newspaper-articles-.html", "13 Detecting text reuse in newspaper articles. 13.1 Turn the newspaper sample into a bunch of text documents, one per article 13.2 Load the files as a TextReuseCorpus 13.3 Further reading", " 13 Detecting text reuse in newspaper articles. 19th century newspapers shared text all the time. Sometimes this took the form of credited reports from other titles. For much of the century, newspapers paid the post office to give them a copy of all other titles. Official reused dispatches were not the only way text was reused: advertisements, of course, were placed in multiple titles at the same time, and editors were happy to use snippets, jokes, and so forth Detecting the extent of this reuse is a great use of digital tools. R has a library, textreuse, which allows you to do this reasonably simply. It was intended to be used for plagiarism detection and to find duplicated documents, but it can also be repurposed to find shared articles. Some of the most inspiring news data projects at the moment are looking at text reuse. The Oceanic Exchanges project is a multi-partner project using various methods to detect this overlap. This methods paper is really interesting, and used a similar starting point, though it then does an extra step of calculating ‘local alignment’ with each candidate pair, to improve the accuracy.15 Melodee Beals’s Scissors and Paste project, at Loughborough and also part of Oceanic Exchanges, also looks at text reuse in 19th century British newspapers. Another project, looking at Finnish newspapers, used a technique usually used to detect protein strains to find clusters of text reuse on particularly inaccurate OCR.16 The steps are the following: Turn the newspaper sample into a bunch of text documents, one per article Load these into R as a special forat called a TextReuseCorpus. Divide the text into a series of overlapping sequences of words, known as n-grams. ‘Hash’ the n-grams - each one is given a numerical code, which is much less memory-hungry. Randomly select 200 of these hashes to represent each document. Use a local sensitivity hashing algorithm (I’ll explain a bit below) to generate a list of potential candidates for text reuse Calculate the similarity scores for these candidates Calculate the local alignment of the pairs to find out exactly which bits overlap To set some expectations: this tutorial uses a small sample dataset of one title over a period of months, and unsurprisingly, there’s not really any text re-use. A larger corpus over a short time period, with a number of titles, would probably give more interesting results. Also, these techniques were developed with modern text in mind, and so the results will be limited by the accurary of the OCR, but by setting the parameters reasonably loose we might be able to mitigate for this. http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html http://infolab.stanford.edu/~ullman/mmds/ch3.pdf 13.1 Turn the newspaper sample into a bunch of text documents, one per article Load libaries: the usual suspect, tidyverse, and also the package ‘textreuse’. If it’s not installed, you’ll need to do so using install.packages('textreuse') library(tidyverse) library(textreuse) ## ## Attaching package: &#39;textreuse&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## tokenize 13.1.1 Load the dataframe and preprocess In the extract text chapter ??, you created a dataframe, with one row per article. The first step is to reload that dataframe into memory, and do some minor preprocessing. load(&#39;news_sample_dataframe&#39;) Make a more useful code to use as an article ID. First use str_pad() to add leading zeros up to a maximum of three digits. news_sample_dataframe$article_code = str_pad(news_sample_dataframe$article_code, width = 3, pad = &#39;0&#39;) Use paste0() to add the prefix ‘article’ to this number. news_sample_dataframe$article_code = paste0(&#39;article_&#39;, news_sample_dataframe$article_code) Unfortunately, this is a very slow process, and we have 170,000 articles to compare. For the purposes of this demonstration, use sample_n to make a sample. The only problem is that it makes the code unreproducible. It might be better to sample based on a limited time period. sample_for_text_reuse = news_sample_dataframe %&gt;% sample_n(10000) 13.1.2 Make a text file from each article This is a very simple function - it says, for each row in the news_sample_dataframe, write the third cell (which is where the text of the article is stored), using a function from a library called data.table called fwrite(), store it in a folder called textfiles/, and make a filename from the article code concatenated with ‘.txt’. R won’t let you create a folder, so create an empty folder first, in the project directory, called textfiles Now you should have a folder in the project folder called textfiles, with a small text document for each article inside. This is a LOT of text documents, so your computer might complain. library(data.table) for(i in 1:nrow(sample_for_text_reuse)){ filename = paste0(&quot;textfiles/&quot;, news_sample_dataframe[i,1],&quot;.txt&quot;) fwrite(news_sample_dataframe[i,3], file = filename) } 13.2 Load the files as a TextReuseCorpus 13.2.1 Generate a minhash Use the function minhash_generator() to specify the number of minhashes you want to represent each document. Set the random seed to make it reproducible. minhash &lt;- minhash_generator(n = 400, seed = 1234) 13.2.2 Create the TextReuseCorpus TextReuseCorpus() takes a number of arguments. Going through each in turn: dir = is the directory where all the text files are stored. tokenizer is the function which tokenises the text. Here we’ve used tokenize_ngrams, but it could also be tokenize words. You could build your own: for example, if you thought that comparing similar characters in small sequences would help to detect text reuse, you could use that to compare the documents. n is the number of tokens in the ngram tokeniser. Setting it at 4 turns the following sentence: Here we’ve used tokenize_ngrams, but it could also be tokenize words into: Here we’ve used tokenize_ngrams we’ve used tokenize_ngrams but used tokenize_ngrams but it tokenize_ngrams but it could but it could also it could also be could also be tokenize also be tokenize words minhash_func = is the parameters set using minhash_generator() above keep_tokens = Whether or not you keep the actual tokens, or just the hashes. There’s no real point keeping the tokens as we use the hashes to make the comparisons. This function will take a long time to run with a large number of documents. reusecorpus &lt;- TextReuseCorpus(dir = &quot;textfiles/&quot;, tokenizer = tokenize_ngrams, n = 3, minhash_func = minhash, keep_tokens = FALSE, progress = FALSE) Now each document is represented by a series of hashes, which are substitutes for small sequences of text. For example, this is the first ten minhashes for the first article: head(minhashes(reusecorpus[[1]]),10) ## [1] -1212289576 -1296295640 -1240828060 -1301884635 -241702409 -1910996997 ## [7] -1463281870 -1859245701 -2022216262 -1370034986 At this point, you could compare any document’s sequences of hashes to any other, and get its ‘Jacquard Similarity’ score, which counts the number of shared hashes in the documents. The more shared hashes, the higher the similarity. However, it would be very difficult, even for a computer, to use this to compare every document to every other in a corpus. A Local Sensitivity Hashing algorithm is used to solve this problem. This groups the representations together, and finds pairs of documents that should be compared for similarity. LSH breaks the minhashes into a series of bands comprised of rows. For example, 200 minhashes might broken into 50 bands of 4 rows each. Each band is hashed to a bucket. If two documents have the exact same minhashes in a band, they will be hashed to the same bucket, and so will be considered candidate pairs. Each pair of documents has as many chances to be considered a candidate as their are bands, and the fewer rows there are in each band, the more likely it is that each document will match another. (https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html) First create the buckets. You can try other values for the bands. buckets &lt;- lsh(reusecorpus, bands = 80, progress = FALSE) Next, use lsh_candidates() to compare each bucket, and generate a list of candidates. candidates &lt;- lsh_candidates(buckets) Next we go back to the full corpus, and calculate the similarity score for these pairs, using lsh_compare(). The first argument is the candidates, the second is the full corpus, the third is the method (other similarity functions could be used). jacsimilarity_both = lsh_compare(candidates, reusecorpus, jaccard_similarity, progress = FALSE) %&gt;% arrange(desc(score)) jacsimilarity_both ## # A tibble: 7,423 x 3 ## a b score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 article_1053 article_1107 1 ## 2 article_1083 article_3348 1 ## 3 article_1110 article_1134 1 ## 4 article_1110 article_1536 1 ## 5 article_1110 article_1566 1 ## 6 article_1110 article_1689 1 ## 7 article_1110 article_1766 1 ## 8 article_1110 article_1827 1 ## 9 article_1110 article_1836 1 ## 10 article_1110 article_1908 1 ## # … with 7,413 more rows It returns a similarity score for each pair: The first pair have a 25% overlap, and the second a much smaller number. The last thing is to join up the article codes to the full text dataset, and actually see what pairs have been detected. This is done using two left_join() commands, one for a and one for b. Also select just the relevant columns, and filter out those with a perfect score as they are very likely to be artefacts rather than full articles, and filter out those where both documents are from the same issue. matchedtexts = jacsimilarity_both %&gt;% left_join(news_sample_dataframe, by = c(&#39;a&#39; = &#39;article_code&#39;)) %&gt;% left_join(news_sample_dataframe, by = c(&#39;b&#39; = &#39;article_code&#39;))%&gt;% select(a,b,score, text.x, text.y, title.x, title.y, full_date.x, full_date.y) matchedtexts = matchedtexts %&gt;% filter(score&lt;1) %&gt;% filter(full_date.x != full_date.y) To check the specific overlap of two documnets, use another function from textreuse to check the ‘local alignment’. This is like comparing two documents in Microsoft Word: it finds the bit of the text with the most overlap, and it points out where in this overlap there are different words, replacing them with ###### First turn the text in each cell into a string: a = paste(matchedtexts$text.x[3], sep=&quot;&quot;, collapse=&quot;&quot;) b = paste(matchedtexts$text.y[3], sep=&quot;&quot;, collapse=&quot;&quot;) Call the align_local() function, giving it the two strings to compare. align_local(a, b) ## TextReuse alignment ## Alignment score: 165 ## Document A: ## TO CORRESPONDENTS No notice can be taken of anonymous communications ## Whatever is intended for insertion must be authenticated by the name ## and address of the writer not necessarily for publication ############ ## but as a guarrantee of his good faith We cannot undertake to return ## rejected communications All communications Books for Review etc to be ## for forwarded warded forwarded to the Editor must be addressed to the ## Publisher CHARLES WILLMER and those from London may be sent to the care ## of Messrs Simpkin Marshall and Co ## ## Document B: ## TO CORRESPONDENTS No notice can be taken of anonymous communications ## Whatever is intended for insertion must be authenticated by the name ## and address of the writer not necessarily for ########### _publication ## but as a guarrantee of his good faith We cannot undertake to return ## rejected communications All communications Books for Review etc to be ## for forwarded warded forwarded to the Editor must be addressed to the ## Publisher CHARLES WILLMER and those from London may be sent to the care ## of Messrs Simpkin Marshall and Co Unsurprisingly, the pairs of documents are nearly all advertisements. This is very much a beginning, but I hope you can see the potential. It’s worth noting that the article segmentation in these newspapers might actually work against the process, because it often lumps multiple articles into one document. Consequently, the software won’t find potential matches if there’s too much other non-matching same text in the same document. A potential work-around would be to split the document into chunks of text, and compare these chunks. The chunks could be joined back to the full articles, and using local alignment, the specific bits that overlapped could be found. 13.3 Further reading David A. Smith, Ryan Cordell, and Abby Mullen, ‘Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers’, American Literary History, 27.3 (2015), E1–E15 &lt;https://doi.org/10.1093/alh/ajv029&gt;.↩ ???, @inproceedings-blast.↩ "],
["further-reading-2.html", "14 Further reading", " 14 Further reading There is a huge volume of literature on R, text analysis and newspaper digitisation. This is a small collection of recommended reading. A useful list of coding resources: https://scottbot.net/teaching-yourself-to-code-in-dh/ A book on R specifically for digital humanities: http://dh-r.lincolnmullen.com Geocomputation with R - a fantastic introduction to advanced mapping and spatial analysis. https://bookdown.org/robinlovelace/geocompr/ Use R to write blog posts: https://bookdown.org/yihui/blogdown/ R-Studio cheatsheets, which are really useful to print out and keep while you’re coding, particularly at the beginning: https://rstudio.com/resources/cheatsheets/ Text mining with R - lots of the examples in this book are based on lessons from here: https://www.tidytextmining.com The best introduction to R and the Tidyverse: https://r4ds.had.co.nz A recent report on newspaper digitisation and metadata standards: https://melissaterras.files.wordpress.com/2020/01/selectioncriterianewspapers_hauswedell_nyhan_beals_terras_bell-3.pdf "],
["appendix-maps-digitised-newspapers-around-the-world.html", "15 Appendix: Maps Digitised Newspapers around the World 15.1 United States", " 15 Appendix: Maps Digitised Newspapers around the World Many national libraries now publish lists of newspapers. As I find them, I’ll add interactive maps here. ## United Kingdom Based on a ‘live’ title list provided by the British Newspaper Archive: ## Warning in validateCoords(lng, lat, funcName): Data contains 1 rows with either ## missing or invalid lat/lon values and will be ignored ## Warning in is.na(values): is.na() applied to non-(list or vector) of type ## &#39;closure&#39; Figure 15.1: Interactive map of the JISC Newspapers 15.1 United States Based on a list of newspapers published here. ## Warning: sf layer has inconsistent datum (+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs ). ## Need &#39;+proj=longlat +datum=WGS84&#39; Figure 15.2: Interactive map of Library of Congress Newspapers "]
]
